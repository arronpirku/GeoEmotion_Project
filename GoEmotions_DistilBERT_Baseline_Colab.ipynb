{"nbformat": 4, "nbformat_minor": 5, "metadata": {"colab": {"name": "GoEmotions_DistilBERT_Baseline_Colab.ipynb", "provenance": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# GoEmotions \u2013 DistilBERT Baseline (Colab)\n", "*Generated on 2025-09-28 11:50 UTC*\n", "\n", "This notebook trains a **multi-label** emotion classifier on **GoEmotions** using Hugging Face Transformers.\n", "It supports both the **raw (27 + neutral)** and **simplified (6 + neutral)** schema, auto-creates a validation split if needed, and can generate a one-click **PDF report** with metrics and per-label F1 charts.\n", "\n", "**What you get:**\n", "- Reproducible baseline (DistilBERT) with simple args\n", "- Micro/Macro/Weighted F1 on validation & test\n", "- Optional threshold sweep to optimize Macro-F1\n", "- Efficiency snapshot (trainable params, basic latency)\n", "- Auto-generated PDF report"]}, {"cell_type": "code", "metadata": {"id": "adf2ea49-faff-4f82-8d6a-1ad9fced839c"}, "execution_count": null, "outputs": [], "source": ["import sys, platform, torch\n", "\n", "print(\"Python:\", sys.version)\n", "print(\"PyTorch:\", torch.__version__)\n", "print(\"CUDA available:\", torch.cuda.is_available())\n", "if torch.cuda.is_available():\n", "    print(\"GPU:\", torch.cuda.get_device_name(0))\n", "else:\n", "    print(\"Running on CPU \u2013 training will be slower.\")"]}, {"cell_type": "code", "metadata": {"id": "185f7e2c-73d9-4df6-8d22-76b3a6c2c482"}, "execution_count": null, "outputs": [], "source": ["!pip -q install transformers==4.44.2 datasets==2.21.0 scikit-learn==1.5.1                accelerate==0.34.2 pandas==2.2.2 matplotlib==3.9.2 reportlab==4.2.2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Configure run\n", "Update the arguments below as needed. Common tweaks:\n", "- `dataset_config`: `\"raw\"` vs `\"simplified\"`\n", "- `epochs`: try 3\u20135 for a quick baseline\n", "- `eval_threshold`: try 0.2\u20130.7 if you *don\u2019t* run the sweep"]}, {"cell_type": "code", "metadata": {"id": "e682967d-4872-4f5a-a390-fbcee1196426"}, "execution_count": null, "outputs": [], "source": ["from dataclasses import dataclass\n", "\n", "@dataclass\n", "class Args:\n", "    model_name: str = \"distilbert-base-uncased\"\n", "    dataset_name: str = \"go_emotions\"\n", "    dataset_config: str = \"raw\"   # \"raw\" or \"simplified\"\n", "    max_length: int = 128\n", "    batch_size: int = 32\n", "    epochs: int = 3\n", "    lr: float = 5e-5\n", "    weight_decay: float = 0.01\n", "    output_dir: str = \"/content/outputs_distilbert_goemotions\"\n", "    seed: int = 42\n", "    eval_threshold: float = 0.5\n", "    report_to: str = \"none\"\n", "    val_frac: float = 0.1\n", "\n", "args = Args()\n", "args"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train & Evaluate"]}, {"cell_type": "code", "metadata": {"id": "3ff9cecd-e950-4f9f-b1b2-8427ee6b7867"}, "execution_count": null, "outputs": [], "source": ["import os, json, time\n", "from typing import List\n", "\n", "import numpy as np\n", "import torch\n", "from datasets import load_dataset, DatasetDict, Sequence\n", "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n", "                          DataCollatorWithPadding, Trainer, TrainingArguments)\n", "from sklearn.metrics import f1_score, classification_report\n", "\n", "RAW_EMOTIONS = [\n", "    \"admiration\",\"amusement\",\"anger\",\"annoyance\",\"approval\",\"caring\",\"confusion\",\n", "    \"curiosity\",\"desire\",\"disappointment\",\"disapproval\",\"disgust\",\"embarrassment\",\n", "    \"excitement\",\"fear\",\"gratitude\",\"grief\",\"joy\",\"love\",\"nervousness\",\"optimism\",\n", "    \"pride\",\"realization\",\"relief\",\"remorse\",\"sadness\",\"surprise\",\"neutral\"\n", "]\n", "SIMPLIFIED_EMOTIONS = [\"anger\",\"disgust\",\"fear\",\"joy\",\"sadness\",\"surprise\",\"neutral\"]\n", "\n", "def ensure_validation(ds: DatasetDict, seed: int, val_frac: float) -> DatasetDict:\n", "    if \"validation\" in ds:\n", "        return ds\n", "    split = ds[\"train\"].train_test_split(test_size=val_frac, seed=seed, stratify_by_column=None)\n", "    return DatasetDict(train=split[\"train\"], validation=split[\"test\"], test=ds[\"test\"] if \"test\" in ds else split[\"test\"])\n", "\n", "def detect_schema_and_label_cols(ds, config: str):\n", "    cols = ds[\"train\"].column_names\n", "    if \"labels\" in cols:\n", "        return \"list\", None\n", "    expected = RAW_EMOTIONS if config == \"raw\" else SIMPLIFIED_EMOTIONS\n", "    label_cols = [c for c in expected if c in cols]\n", "    if label_cols:\n", "        return \"wide\", label_cols\n", "    raise KeyError(f\"Could not detect labels. Columns found: {cols}\")\n", "\n", "def get_label_names(ds, schema: str, label_cols, config: str) -> List[str]:\n", "    if schema == \"list\":\n", "        feat = ds[\"train\"].features[\"labels\"]\n", "        if isinstance(feat, Sequence):\n", "            return feat.feature.names\n", "        # Fallback: infer max label id\n", "        max_id = 0\n", "        for ex in ds[\"train\"][\"labels\"][:1000]:\n", "            if isinstance(ex, list) and len(ex) > 0:\n", "                max_id = max(max_id, max(ex))\n", "        return [str(i) for i in range(max_id + 1)]\n", "    else:\n", "        return label_cols\n", "\n", "def attach_labels(examples, schema: str, label_names: List[str], label_cols=None):\n", "    n = len(examples[\"text\"])\n", "    y = np.zeros((n, len(label_names)), dtype=np.float32)\n", "    if schema == \"list\":\n", "        for i, lbls in enumerate(examples[\"labels\"]):\n", "            for j in lbls:\n", "                if 0 <= j < len(label_names):\n", "                    y[i, j] = 1.0\n", "    else:\n", "        for idx, name in enumerate(label_names):\n", "            y[:, idx] = np.array(examples[name], dtype=np.float32)\n", "    return {\"labels\": y.tolist()}\n", "\n", "def compute_metrics_builder(threshold: float, label_names: List[str], out_dir: str):\n", "    def compute_metrics(eval_pred):\n", "        logits, labels = eval_pred\n", "        probs = 1 / (1 + np.exp(-logits))\n", "        preds = (probs >= threshold).astype(int)\n", "        micro = f1_score(labels, preds, average=\"micro\", zero_division=0)\n", "        macro = f1_score(labels, preds, average=\"macro\", zero_division=0)\n", "        weighted = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n", "        rep = classification_report(labels, preds, target_names=label_names, zero_division=0, output_dict=True)\n", "        with open(os.path.join(out_dir, \"classification_report.json\"), \"w\") as f:\n", "            json.dump(rep, f, indent=2)\n", "        return {\"f1_micro\": float(micro), \"f1_macro\": float(macro), \"f1_weighted\": float(weighted)}\n", "    return compute_metrics\n", "\n", "def count_trainable_parameters(model: torch.nn.Module) -> int:\n", "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n", "\n", "def measure_latency(model, tokenizer, device: str, max_length: int = 128, batch_size: int = 32, iters: int = 30):\n", "    model.eval()\n", "    sents = [\"This is a sample sentence about feelings.\"] * batch_size\n", "    with torch.no_grad():\n", "        # warmup\n", "        for _ in range(5):\n", "            _ = model(**tokenizer(sents, return_tensors=\"pt\", padding=True, truncation=True,\n", "                                  max_length=max_length).to(device))\n", "        start = time.time()\n", "        for _ in range(iters):\n", "            _ = model(**tokenizer(sents, return_tensors=\"pt\", padding=True, truncation=True,\n", "                                  max_length=max_length).to(device))\n", "        end = time.time()\n", "    return (end - start) * 1000.0 / iters\n", "\n", "# Set seeds & dirs\n", "os.makedirs(args.output_dir, exist_ok=True)\n", "torch.manual_seed(args.seed)\n", "np.random.seed(args.seed)\n", "\n", "# Load dataset\n", "ds = load_dataset(args.dataset_name, args.dataset_config)\n", "ds = ensure_validation(ds, seed=args.seed, val_frac=args.val_frac)\n", "\n", "# Schema + labels\n", "schema, label_cols = detect_schema_and_label_cols(ds, args.dataset_config)\n", "label_names = get_label_names(ds, schema, label_cols, args.dataset_config)\n", "\n", "# Tokenizer & preprocess\n", "tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)\n", "\n", "def preprocess(batch):\n", "    enc = tokenizer(batch[\"text\"], truncation=True, max_length=args.max_length)\n", "    enc.update(attach_labels(batch, schema, label_names, label_cols))\n", "    return enc\n", "\n", "remove_cols = [c for c in ds[\"train\"].column_names if c != \"text\"]\n", "encoded = ds.map(preprocess, batched=True, remove_columns=remove_cols)\n", "\n", "# Collator & model\n", "collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8 if torch.cuda.is_available() else None)\n", "model = AutoModelForSequenceClassification.from_pretrained(\n", "    args.model_name,\n", "    num_labels=len(label_names),\n", "    problem_type=\"multi_label_classification\",\n", "    id2label={i: n for i, n in enumerate(label_names)},\n", "    label2id={n: i for i, n in enumerate(label_names)},\n", ")\n", "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "model.to(device)\n", "\n", "targs = TrainingArguments(\n", "    output_dir=args.output_dir,\n", "    per_device_train_batch_size=args.batch_size,\n", "    per_device_eval_batch_size=args.batch_size,\n", "    num_train_epochs=args.epochs,\n", "    learning_rate=args.lr,\n", "    weight_decay=args.weight_decay,\n", "    logging_steps=100,\n", "    report_to=args.report_to,\n", "    seed=args.seed,\n", ")\n", "\n", "trainer = Trainer(\n", "    model=model,\n", "    args=targs,\n", "    train_dataset=encoded[\"train\"],\n", "    eval_dataset=encoded[\"validation\"],\n", "    tokenizer=tokenizer,\n", "    data_collator=collator,\n", "    compute_metrics=compute_metrics_builder(args.eval_threshold, label_names, args.output_dir),\n", ")\n", "\n", "train_metrics = trainer.train()\n", "val_metrics = trainer.evaluate()\n", "test_metrics = trainer.evaluate(encoded[\"test\"]) if \"test\" in encoded else {}\n", "\n", "with open(os.path.join(args.output_dir, \"metrics.json\"), \"w\") as f:\n", "    json.dump({\"train\": train_metrics.metrics, \"val\": val_metrics, \"test\": test_metrics,\n", "               \"label_names\": label_names, \"args\": vars(args)}, f, indent=2)\n", "\n", "params = count_trainable_parameters(model)\n", "latency_ms = measure_latency(model, tokenizer, device=device, max_length=args.max_length, batch_size=32, iters=30)\n", "with open(os.path.join(args.output_dir, \"efficiency_snapshot.json\"), \"w\") as f:\n", "    json.dump({\"trainable_params\": int(params), \"avg_latency_ms_per_batch32\": float(latency_ms)}, f, indent=2)\n", "\n", "print(\"Done. Saved outputs in:\", args.output_dir)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## (Optional) Threshold sweep for Macro-F1\n", "Run this to find a better decision threshold on the **validation** set. It reads logits & labels from the last eval pass (re-computes if needed)."]}, {"cell_type": "code", "metadata": {"id": "1a5a4464-155e-494a-9210-c7a73161631a"}, "execution_count": null, "outputs": [], "source": ["import os, json, numpy as np\n", "from sklearn.metrics import f1_score\n", "\n", "# Re-run eval to get logits & labels\n", "eval_out = trainer.predict(encoded[\"validation\"])\n", "logits = eval_out.predictions\n", "labels = eval_out.label_ids\n", "probs = 1/(1+np.exp(-logits))\n", "\n", "def eval_at(th):\n", "    preds = (probs >= th).astype(int)\n", "    return dict(\n", "        micro = f1_score(labels, preds, average=\"micro\", zero_division=0),\n", "        macro = f1_score(labels, preds, average=\"macro\", zero_division=0),\n", "        weighted = f1_score(labels, preds, average=\"weighted\", zero_division=0),\n", "        th = th\n", "    )\n", "\n", "ths = np.round(np.linspace(0.1, 0.9, 17), 3)\n", "scores = [eval_at(th) for th in ths]\n", "best = max(scores, key=lambda d: d[\"macro\"])\n", "\n", "print(\"Best (by Macro-F1):\", best)\n", "with open(os.path.join(args.output_dir, \"threshold_sweep.json\"), \"w\") as f:\n", "    json.dump({\"scores\": scores, \"best\": best}, f, indent=2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Generate PDF report"]}, {"cell_type": "code", "metadata": {"id": "c6648246-7afb-43de-aa3d-80d65f7c6bbe"}, "execution_count": null, "outputs": [], "source": ["import os, json\n", "from datetime import datetime\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from reportlab.lib.pagesizes import A4\n", "from reportlab.lib.units import cm\n", "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle\n", "from reportlab.lib.styles import getSampleStyleSheet\n", "from reportlab.lib import colors\n", "\n", "outputs_dir = args.output_dir\n", "metrics_path = os.path.join(outputs_dir, \"metrics.json\")\n", "report_path  = os.path.join(outputs_dir, \"classification_report.json\")\n", "eff_path     = os.path.join(outputs_dir, \"efficiency_snapshot.json\")\n", "\n", "assert os.path.exists(metrics_path), \"metrics.json not found\"\n", "assert os.path.exists(report_path), \"classification_report.json not found\"\n", "assert os.path.exists(eff_path), \"efficiency_snapshot.json not found\"\n", "\n", "METRICS = json.load(open(metrics_path))\n", "REPORT  = json.load(open(report_path))\n", "EFF     = json.load(open(eff_path))\n", "\n", "def safe_get(metric_block, keys):\n", "    for k in keys:\n", "        if k in metric_block and metric_block[k] is not None:\n", "            return metric_block[k]\n", "    return None\n", "\n", "val = METRICS.get(\"val\", {}) or {}\n", "test = METRICS.get(\"test\", {}) or {}\n", "val_micro = safe_get(val, [\"f1_micro\", \"eval_f1_micro\"])\n", "val_macro = safe_get(val, [\"f1_macro\", \"eval_f1_macro\"])\n", "val_weighted = safe_get(val, [\"f1_weighted\", \"eval_f1_weighted\"])\n", "val_loss = safe_get(val, [\"loss\", \"eval_loss\"])\n", "t_micro = safe_get(test, [\"f1_micro\", \"eval_f1_micro\"])\n", "t_macro = safe_get(test, [\"f1_macro\", \"eval_f1_macro\"])\n", "t_weighted = safe_get(test, [\"f1_weighted\", \"eval_f1_weighted\"])\n", "t_loss = safe_get(test, [\"loss\", \"eval_loss\"])\n", "\n", "label_names = METRICS.get(\"label_names\", [])\n", "cfg = METRICS.get(\"args\", {})\n", "\n", "# Build top/bottom-10 chart\n", "rows = []\n", "for lbl in label_names:\n", "    stats = REPORT.get(lbl, {})\n", "    if isinstance(stats, dict) and \"f1-score\" in stats:\n", "        rows.append({\"label\": lbl,\n", "                     \"precision\": stats.get(\"precision\", 0.0),\n", "                     \"recall\": stats.get(\"recall\", 0.0),\n", "                     \"f1\": stats.get(\"f1-score\", 0.0),\n", "                     \"support\": stats.get(\"support\", 0)})\n", "chart_path = None\n", "if rows:\n", "    df = pd.DataFrame(rows).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n", "    top = df.head(10); bottom = df.tail(10)\n", "    labels = list(top[\"label\"]) + [\"...\"] + list(bottom[\"label\"])\n", "    values = list(top[\"f1\"]) + [None] + list(bottom[\"f1\"])\n", "    x = list(range(len(values)))\n", "    sep_idx = labels.index(\"...\")\n", "    plt.figure(figsize=(7,5))\n", "    plt.bar(x[:sep_idx], values[:sep_idx])\n", "    plt.bar(x[sep_idx+1:], values[sep_idx+1:])\n", "    plt.xticks(x, labels, rotation=45, ha=\"right\")\n", "    plt.title(\"Per-label F1 (Top-10 & Bottom-10)\")\n", "    plt.tight_layout()\n", "    chart_path = os.path.join(outputs_dir, \"label_f1_chart.png\")\n", "    plt.savefig(chart_path, dpi=200)\n", "    plt.close()\n", "\n", "# Build PDF\n", "pdf_path = os.path.join(outputs_dir, \"goemotions_baseline_report.pdf\")\n", "styles = getSampleStyleSheet()\n", "title, h2, body = styles[\"Title\"], styles[\"Heading2\"], styles[\"BodyText\"]\n", "doc = SimpleDocTemplate(pdf_path, pagesize=A4, leftMargin=2*cm, rightMargin=2*cm, topMargin=1.5*cm, bottomMargin=1.5*cm)\n", "story = []\n", "story.append(Paragraph(\"GoEmotions Baseline Report\", title))\n", "story.append(Paragraph(datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\"), body)); story.append(Spacer(1,12))\n", "\n", "summary = f\"\"\"\n", "<b>Model:</b> {cfg.get('model_name')}<br/>\n", "<b>Dataset:</b> {cfg.get('dataset_name')} ({cfg.get('dataset_config')})<br/>\n", "<b>Training:</b> epochs={cfg.get('epochs', cfg.get('num_train_epochs', 3))},\n", "batch_size={cfg.get('batch_size', cfg.get('per_device_train_batch_size', 32))},\n", "max_length={cfg.get('max_length', 128)},\n", "threshold={cfg.get('eval_threshold', 0.5)}<br/>\n", "\"\"\"\n", "story.append(Paragraph(\"Run Summary\", h2))\n", "story.append(Paragraph(summary, body)); story.append(Spacer(1,8))\n", "\n", "def fmt(x):\n", "    try: return f\"{float(x):.4f}\"\n", "    except: return \"nan\"\n", "\n", "tbl = [\n", "    [\"\", \"F1 (micro)\", \"F1 (macro)\", \"F1 (weighted)\", \"Loss\"],\n", "    [\"Validation\", fmt(val_micro), fmt(val_macro), fmt(val_weighted), fmt(val_loss)],\n", "    [\"Test\", fmt(t_micro), fmt(t_macro), fmt(t_weighted), fmt(t_loss)],\n", "]\n", "table = Table(tbl, hAlign=\"LEFT\")\n", "table.setStyle(TableStyle([\n", "    (\"BACKGROUND\", (0,0), (-1,0), colors.lightgrey),\n", "    (\"GRID\", (0,0), (-1,-1), 0.25, colors.grey),\n", "    (\"FONTNAME\", (0,0), (-1,0), \"Helvetica-Bold\"),\n", "    (\"ALIGN\", (1,1), (-1,-1), \"CENTER\"),\n", "]))\n", "story.append(Paragraph(\"Evaluation Metrics\", h2)); story.append(table); story.append(Spacer(1,8))\n", "\n", "eff_txt = f\"\"\"\n", "<b>Trainable parameters:</b> {EFF.get('trainable_params','?'):,}<br/>\n", "<b>Avg latency (ms) per batch of 32:</b> {EFF.get('avg_latency_ms_per_batch32','?'):.2f}\n", "\"\"\"\n", "story.append(Paragraph(\"Efficiency Snapshot\", h2))\n", "story.append(Paragraph(eff_txt, body)); story.append(Spacer(1,8))\n", "\n", "if chart_path and os.path.exists(chart_path):\n", "    story.append(Paragraph(\"Per-label F1 \u2014 Top & Bottom 10\", h2))\n", "    story.append(Image(chart_path, width=16*cm, height=10*cm)); story.append(Spacer(1,8))\n", "\n", "notes = \"\"\"\n", "<b>Notes:</b><br/>\n", "\u2022 Fixed decision threshold by default. Run the threshold sweep cell to optimize Macro-F1.<br/>\n", "\u2022 Latency is a simple forward pass benchmark; serving latency varies by hardware and batch size.\n", "\"\"\"\n", "story.append(Paragraph(\"Notes\", h2))\n", "story.append(Paragraph(notes, body))\n", "\n", "doc.build(story)\n", "print(\"PDF written to:\", pdf_path)"]}]}