{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc6b6073"
      },
      "source": [
        "### Overview of Key Configuration Flags\n",
        "\n",
        "This section summarizes the most important settings in the notebook, which control the execution mode and workflow.\n",
        "\n",
        "*   **`SMOKE_TEST = False`** (cell_id: `YwTG0SYEswsn`)\n",
        "    *   `True`: Fast testing mode, where processes run only on a configured mini-dataset. Ideal for quick code validation, but not suitable for generating real results.\n",
        "    *   `False`: Full execution mode, where the notebook works with the entire dataset. This can take a long time and utilizes GPU resources.\n",
        "\n",
        "*   **`BASE_MODELS` list** (cell_id: `tI_vS_FQqqD9`)\n",
        "    *   This list defines the base models you want to use. For each model, you can set:\n",
        "        *   `\"hpo\": True/False`: Whether Hyperparameter Optimization (HPO) should run for the given model.\n",
        "        *   `\"train\": True/False`: Whether the final training should run for the given model with the best HPO configuration.\n",
        "    *   **Important:** If you want to run full HPO and training, set these flags to `True` for the desired models. For example:\n",
        "        ```python\n",
        "        BASE_MODELS = [\n",
        "            {\"name\": \"nreimers/MiniLM-L6-H384-uncased\", \"hpo\": True, \"hpo_mode\": RAW_CONFIG_NAME, \"train\": True, \"train_mode\": RAW_CONFIG_NAME},\n",
        "            # ...\n",
        "        ]\n",
        "        ```\n",
        "\n",
        "*   **`RUN_QUANTIZATION = False`** (cell_id: `tI_vS_FQqqD9`)\n",
        "    *   `True`: Enables the execution of the quantization and final evaluation of quantized models.\n",
        "    *   `False`: Skips the quantization phase.\n",
        "    *   **Note:** Quantization should ideally be run after HPO and final training are complete.\n",
        "\n",
        "*   **`GENERATE_FINAL_REPORT = False`** (cell_id: `tI_vS_FQqqD9`)\n",
        "    *   `True`: Enables the generation of the final PDF report.\n",
        "    *   `False`: Skips report generation.\n",
        "    *   **Note:** The report will only contain all results if all previous steps (HPO, training, quantization) have been completed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tI_vS_FQqqD9"
      },
      "outputs": [],
      "source": [
        "RAW_CONFIG_NAME = \"raw\"\n",
        "SIMPLIFIED_CONFIG_NAME = \"simplified\"\n",
        "\n",
        "BASE_MODELS = [\n",
        "    {\"name\": \"nreimers/MiniLM-L6-H384-uncased\", \"hpo\": False, \"hpo_mode\": RAW_CONFIG_NAME, \"train\": False, \"train_mode\": RAW_CONFIG_NAME, \"trained\": False},\n",
        "    {\"name\": \"google/electra-small-discriminator\", \"hpo\": False, \"hpo_mode\": RAW_CONFIG_NAME, \"train\": False, \"train_mode\": RAW_CONFIG_NAME, \"trained\": False},\n",
        "    {\"name\": \"roberta-base\", \"hpo\": False, \"hpo_mode\": RAW_CONFIG_NAME, \"train\": False, \"train_mode\": RAW_CONFIG_NAME, \"trained\": False},\n",
        "]\n",
        "\n",
        "RUN_QUANTIZATION = False    # <<< CHANGE TO TRUE when all training has been done after all HPO has been done\n",
        "\n",
        "GENERATE_FINAL_REPORT = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YwTG0SYEswsn"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# RUN MODE: SMOKE TEST vs FULL\n",
        "# ====================================================\n",
        "\n",
        "SMOKE_TEST = True    # True = tiny debugging subset, False = full HPO\n",
        "MINI_TRAIN_SIZE = 50\n",
        "MINI_VALID_SIZE = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b258AgTpTYQU",
        "outputId": "728c4d49-94cd-4cc2-b120-b9bd0a8a570b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tokenizers 0.22.1\n",
            "Uninstalling tokenizers-0.22.1:\n",
            "  Successfully uninstalled tokenizers-0.22.1\n",
            "Found existing installation: transformers 4.57.1\n",
            "Uninstalling transformers-4.57.1:\n",
            "  Successfully uninstalled transformers-4.57.1\n",
            "Found existing installation: datasets 4.0.0\n",
            "Uninstalling datasets-4.0.0:\n",
            "  Successfully uninstalled datasets-4.0.0\n",
            "\u001b[33mWARNING: Skipping evaluate as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: accelerate 1.11.0\n",
            "Uninstalling accelerate-1.11.0:\n",
            "  Successfully uninstalled accelerate-1.11.0\n",
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, tokenizers, transformers, datasets, accelerate, evaluate\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "Successfully installed accelerate-1.11.0 datasets-4.4.1 evaluate-0.4.6 pyarrow-22.0.0 tokenizers-0.22.1 transformers-4.57.1\n",
            "Mounted at /content/drive\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ====================================================\n",
        "# 0. Environment: install, import, Drive mount, config\n",
        "# ====================================================\n",
        "\n",
        "# ---- Clean install of HF stack (no Python switching needed) ----\n",
        "!pip uninstall -y tokenizers transformers datasets evaluate accelerate\n",
        "!pip cache purge\n",
        "\n",
        "!pip install -U transformers datasets tokenizers evaluate accelerate\n",
        "\n",
        "# ---- Standard imports ----\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# ---- Mount Google Drive ----\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---- HuggingFace imports ----\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from datasets import load_dataset, DatasetDict, load_from_disk, Dataset\n",
        "import evaluate\n",
        "from torch import nn\n",
        "\n",
        "# ====================================================\n",
        "# Directory structure\n",
        "# ====================================================\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/emotion_project\"\n",
        "\n",
        "if SMOKE_TEST:\n",
        "    MODE_ROOT = os.path.join(PROJECT_ROOT, \"mini\")\n",
        "else:\n",
        "    MODE_ROOT = os.path.join(PROJECT_ROOT, \"full\")\n",
        "\n",
        "DATA_CACHE_DIR = os.path.join(MODE_ROOT, \"cache\")\n",
        "TOKENIZED_DIR  = os.path.join(MODE_ROOT, \"tokenized\")\n",
        "TRIALS_DIR     = os.path.join(MODE_ROOT, \"trials\")\n",
        "FINAL_DIR      = os.path.join(MODE_ROOT, \"final_models\")\n",
        "LOGS_DIR       = os.path.join(MODE_ROOT, \"logs\")\n",
        "\n",
        "for d in [PROJECT_ROOT, MODE_ROOT, DATA_CACHE_DIR, TOKENIZED_DIR, TRIALS_DIR, FINAL_DIR, LOGS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# Model and HPO configs\n",
        "# ====================================================\n",
        "\n",
        "MAX_LENGTH = 64\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 6\n",
        "NUM_EPOCHS_HPO = 3\n",
        "SEED = 42\n",
        "\n",
        "# ---- Manual HPO grid ----\n",
        "HPO_GRID = [\n",
        "    {\n",
        "        \"learning_rate\": 2e-5,\n",
        "        \"warmup_ratio\": 0.05,\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"scheduler\": \"linear\",\n",
        "        \"batch_size\": 16,\n",
        "        \"max_length\": 64,\n",
        "        \"optimizer\": \"adamw_torch\",\n",
        "    },\n",
        "    {\n",
        "        \"learning_rate\": 3e-5,\n",
        "        \"warmup_ratio\": 0.1,\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"scheduler\": \"cosine\",\n",
        "        \"batch_size\": 8,\n",
        "        \"max_length\": 128,\n",
        "        \"optimizer\": \"adamw_torch\",\n",
        "    },\n",
        "    {\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"warmup_ratio\": 0.0,\n",
        "        \"weight_decay\": 0.0,\n",
        "        \"scheduler\": \"linear\",\n",
        "        \"batch_size\": 16,\n",
        "        \"max_length\": 64,\n",
        "        \"optimizer\": \"adafactor\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# ---- Reproducibility ----\n",
        "def set_global_seed(seed: int = SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KTYLDQQRTYOD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "7a178eb2-0742-48ca-d65f-e4fc100efc71"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lru_cache' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3150626415.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m#  Load datasets + compute label infos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# ====================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_datasets_and_label_infos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lru_cache' is not defined"
          ]
        }
      ],
      "source": [
        "GOEMOTIONS_28 = [\n",
        "    'admiration','amusement','anger','annoyance','approval','caring',\n",
        "    'confusion','curiosity','desire','disappointment','disapproval','disgust',\n",
        "    'embarrassment','excitement','fear','gratitude','grief','joy','love',\n",
        "    'nervousness','optimism','pride','realization','relief','remorse',\n",
        "    'sadness','surprise'\n",
        "]\n",
        "\n",
        "# ====================================================\n",
        "# RAW label-info (28 labels)\n",
        "# ====================================================\n",
        "def get_label_info_raw(ds: DatasetDict) -> Dict[str, Any]:\n",
        "    train_ds = ds[\"train\"]\n",
        "\n",
        "    label_names = GOEMOTIONS_28\n",
        "    num_labels = len(label_names)\n",
        "\n",
        "    arr = np.stack([train_ds[col] for col in label_names], axis=1).astype(np.float32)\n",
        "    counts = arr.sum(axis=0)\n",
        "\n",
        "    N = len(train_ds)\n",
        "\n",
        "    pos = counts\n",
        "    neg = N - counts\n",
        "    pos_weight = neg / (pos + 1e-6)\n",
        "\n",
        "    id2label = {i: name for i, name in enumerate(label_names)}\n",
        "    label2id = {name: i for i, name in id2label.items()}\n",
        "\n",
        "    return {\n",
        "        \"mode\": \"raw\",\n",
        "        \"num_labels\": num_labels,\n",
        "        \"id2label\": id2label,\n",
        "        \"label2id\": label2id,\n",
        "        \"pos_weight\": torch.tensor(pos_weight, dtype=torch.float32),\n",
        "        \"class_weights\": None,\n",
        "    }\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# SIMPLIFIED label-info (28 labels)\n",
        "# ====================================================\n",
        "def get_label_info_simplified(ds: DatasetDict) -> Dict[str, Any]:\n",
        "    train_ds = ds[\"train\"]\n",
        "    features = train_ds.features\n",
        "\n",
        "    labels_feature = features[\"labels\"]\n",
        "\n",
        "    # Ensure the feature is a Sequence(ClassLabel)\n",
        "    if hasattr(labels_feature, \"feature\") and hasattr(labels_feature.feature, \"names\"):\n",
        "        label_names = labels_feature.feature.names\n",
        "    else:\n",
        "        raise ValueError(\"Simplified 'labels' must be Sequence(ClassLabel).\")\n",
        "\n",
        "    num_labels = len(label_names)\n",
        "\n",
        "    # ---- FAST LABEL COUNTS (for class_weights) ----\n",
        "    N = len(train_ds) # Total number of samples in the training dataset\n",
        "    counts = np.zeros(num_labels, dtype=np.float32)\n",
        "    for example_labels in train_ds[\"labels\"]:\n",
        "        # For simplified, example_labels is typically a list with one element, e.g., [7]\n",
        "        # The actual label ID is example_labels[0]\n",
        "        counts[example_labels[0]] += 1\n",
        "\n",
        "    # Calculate class weights for CrossEntropyLoss (inverse frequency: N_total / N_k)\n",
        "    class_weights_array = np.zeros(num_labels, dtype=np.float32)\n",
        "    for i in range(num_labels):\n",
        "        if counts[i] > 0:\n",
        "            class_weights_array[i] = N / counts[i]\n",
        "        else:\n",
        "            # If a class has no samples, assign it a weight of 0.0 so it doesn't contribute to loss\n",
        "            class_weights_array[i] = 0.0\n",
        "\n",
        "    id2label = {i: n for i, n in enumerate(label_names)}\n",
        "    label2id = {n: i for i, n in id2label.items()}\n",
        "\n",
        "    return {\n",
        "        \"mode\": SIMPLIFIED_CONFIG_NAME,\n",
        "        \"num_labels\": num_labels,\n",
        "        \"id2label\": id2label,\n",
        "        \"label2id\": label2id,\n",
        "        \"pos_weight\": None, # pos_weight is for multi-label, set to None for simplified\n",
        "        \"class_weights\": torch.tensor(class_weights_array, dtype=torch.float32), # Now calculated\n",
        "    }\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# RAW → multi-label conversion (28 labels)\n",
        "# ====================================================\n",
        "def convert_raw_to_multilabel(ds: DatasetDict) -> DatasetDict:\n",
        "    \"\"\"\n",
        "    Converting the RAW GoEmotions dataset (28 binary columns)\n",
        "    → to one multi-hot list 'labels'.\n",
        "    \"\"\"\n",
        "\n",
        "    def build_labels(example):\n",
        "        return {\n",
        "            \"labels\": [int(example[label]) for label in GOEMOTIONS_28]\n",
        "        }\n",
        "\n",
        "    converted = ds.map(build_labels)\n",
        "\n",
        "    # remove original 28 columns\n",
        "    converted = converted.remove_columns(GOEMOTIONS_28)\n",
        "\n",
        "    return converted\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "#  Load datasets + compute label infos\n",
        "# ====================================================\n",
        "@lru_cache()\n",
        "def get_datasets_and_label_infos() -> Tuple[Dict[str, DatasetDict], Dict[str, Dict[str, Any]]]:\n",
        "\n",
        "    set_global_seed(SEED)\n",
        "\n",
        "    print(\"Loading GoEmotions RAW config...\")\n",
        "    raw_full = load_dataset(\"go_emotions\", RAW_CONFIG_NAME)\n",
        "\n",
        "    # Train/validation split\n",
        "    raw_train_valid = raw_full[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
        "    raw_train_valid[\"validation\"] = raw_train_valid.pop(\"test\")\n",
        "\n",
        "    print(\"Loading GoEmotions SIMPLIFIED config...\")\n",
        "    simplified_ds = load_dataset(\"go_emotions\", SIMPLIFIED_CONFIG_NAME)\n",
        "\n",
        "    # MINI slicing (if needed)\n",
        "    if SMOKE_TEST:\n",
        "        raw_train_valid[\"train\"] = raw_train_valid[\"train\"].select(range(MINI_TRAIN_SIZE))\n",
        "        raw_train_valid[\"validation\"] = raw_train_valid[\"validation\"].select(range(MINI_VALID_SIZE))\n",
        "        simplified_ds[\"train\"] = simplified_ds[\"train\"].select(range(MINI_TRAIN_SIZE))\n",
        "        simplified_ds[\"validation\"] = simplified_ds[\"validation\"].select(range(MINI_VALID_SIZE))\n",
        "\n",
        "    print(\"Computing RAW label-info\\u2026 (28 labels)\")\n",
        "    raw_info = get_label_info_raw(raw_train_valid)\n",
        "\n",
        "    print(\"Converting RAW → multi-label (28)\")\n",
        "    raw_ml = convert_raw_to_multilabel(raw_train_valid)\n",
        "\n",
        "    print(\"Computing SIMPLIFIED label-info\\u2026 (20 labels)\")\n",
        "    simplified_info = get_label_info_simplified(simplified_ds)\n",
        "\n",
        "    datasets = {\n",
        "        RAW_CONFIG_NAME: raw_ml,\n",
        "        SIMPLIFIED_CONFIG_NAME: simplified_ds,\n",
        "    }\n",
        "    infos = {\n",
        "        RAW_CONFIG_NAME: raw_info,\n",
        "        SIMPLIFIED_CONFIG_NAME: simplified_info,\n",
        "    }\n",
        "\n",
        "    return datasets, infos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDBi1c3XTYML"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# 2. Tokenization + caching (tokenized dataset saved to Drive)\n",
        "# ====================================================\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Tokenizer loader\n",
        "# ----------------------------------------------------\n",
        "def prepare_tokenizer(model_name: str) -> AutoTokenizer:\n",
        "    return AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# RAW preprocessing (multi-label)\n",
        "# ----------------------------------------------------\n",
        "def preprocess_function_raw(examples, tokenizer, num_labels: int, max_len: int):\n",
        "    texts = examples[\"text\"]\n",
        "\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_len,\n",
        "    )\n",
        "\n",
        "    # examples[\"labels\"] is already multi-hot lists\n",
        "    multi_hot = np.array(examples[\"labels\"], dtype=np.float32)\n",
        "\n",
        "    if multi_hot.shape[1] != num_labels:\n",
        "        raise ValueError(f\"Expected {num_labels} labels, got {multi_hot.shape[1]}\")\n",
        "\n",
        "    encodings[\"labels\"] = multi_hot.tolist()\n",
        "    return encodings\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Simplified preprocessing (single-label)\n",
        "# ----------------------------------------------------\n",
        "def preprocess_function_simplified(examples, tokenizer, max_len: int):\n",
        "    texts = examples[\"text\"]\n",
        "\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_len,\n",
        "    )\n",
        "\n",
        "    # Simplified: labels = [[13], [7], ...]\n",
        "    encodings[\"labels\"] = [lbl_list[0] for lbl_list in examples[\"labels\"]]\n",
        "\n",
        "    return encodings\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Tokenize dataset for a given model\n",
        "# ----------------------------------------------------\n",
        "def tokenize_dataset_for_model(\n",
        "    model_name: str,\n",
        "    ds: DatasetDict,\n",
        "    mode: str,            # \"raw\" or \"simplified\"\n",
        "    label_info: Dict[str, Any],\n",
        "    max_len: int = MAX_LENGTH,\n",
        ") -> Tuple[DatasetDict, AutoTokenizer]:\n",
        "\n",
        "    tokenizer = prepare_tokenizer(model_name)\n",
        "\n",
        "    if mode == RAW_CONFIG_NAME:\n",
        "        fn = lambda ex: preprocess_function_raw(\n",
        "            ex, tokenizer, label_info[\"num_labels\"], max_len\n",
        "        )\n",
        "    else:\n",
        "        fn = lambda ex: preprocess_function_simplified(\n",
        "            ex, tokenizer, max_len\n",
        "        )\n",
        "\n",
        "    # Remove original columns to keep dataset compact\n",
        "    tokenized = ds.map(\n",
        "        fn,\n",
        "        batched=True,\n",
        "        remove_columns=ds[\"train\"].column_names,\n",
        "        desc=f\"Tokenizing for model={model_name}, mode={mode}\",\n",
        "    )\n",
        "\n",
        "    tokenized.set_format(type=\"torch\")\n",
        "    return tokenized, tokenizer\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Cached tokenized dataset loader\n",
        "# ----------------------------------------------------\n",
        "def get_tokenized_dataset_cached(\n",
        "    base_model_name: str,\n",
        "    mode: str,\n",
        "    max_len: int = MAX_LENGTH,\n",
        ") -> Tuple[DatasetDict, AutoTokenizer, Dict[str, Any]]:\n",
        "\n",
        "    datasets_dict, infos_dict = get_datasets_and_label_infos()\n",
        "    ds = datasets_dict[mode]\n",
        "    label_info = infos_dict[mode]\n",
        "\n",
        "    model_key = base_model_name.replace(\"/\", \"_\")\n",
        "    cache_dir = os.path.join(TOKENIZED_DIR, f\"{model_key}_{mode}_len{max_len}\")\n",
        "\n",
        "    # ---- Load from cache ----\n",
        "    if os.path.exists(cache_dir):\n",
        "        print(f\"[CACHE] Loading tokenized dataset from {cache_dir}\")\n",
        "        tokenized = load_from_disk(cache_dir)\n",
        "        tokenizer = prepare_tokenizer(base_model_name)\n",
        "        return tokenized, tokenizer, label_info\n",
        "\n",
        "    # ---- Create new tokenized version ----\n",
        "    print(f\"[TOKENIZE] Creating tokenized dataset for model={base_model_name} | mode={mode}\")\n",
        "    tokenized, tokenizer = tokenize_dataset_for_model(\n",
        "        model_name=base_model_name,\n",
        "        ds=ds,\n",
        "        mode=mode,\n",
        "        label_info=label_info,\n",
        "        max_len=max_len,\n",
        "    )\n",
        "\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    tokenized.save_to_disk(cache_dir)\n",
        "    print(f\"[TOKENIZE] Saved tokenized dataset to {cache_dir}\")\n",
        "\n",
        "    return tokenized, tokenizer, label_info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF7lnYt0TYKM"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# 3. Plutchik-distance metrics + WeightedTrainer\n",
        "# ====================================================\n",
        "\n",
        "# Simple mapping: GoEmotions label → Plutchik primary category\n",
        "RAW_LABEL_TO_PLUTCHIK = {\n",
        "    \"admiration\": \"trust\",\n",
        "    \"amusement\": \"joy\",\n",
        "    \"anger\": \"anger\",\n",
        "    \"annoyance\": \"anger\",\n",
        "    \"approval\": \"trust\",\n",
        "    \"caring\": \"trust\",\n",
        "    \"confusion\": \"surprise\",\n",
        "    \"curiosity\": \"anticipation\",\n",
        "    \"desire\": \"anticipation\",\n",
        "    \"disappointment\": \"sadness\",\n",
        "    \"disapproval\": \"disgust\",\n",
        "    \"disgust\": \"disgust\",\n",
        "    \"embarrassment\": \"sadness\",\n",
        "    \"excitement\": \"joy\",\n",
        "    \"fear\": \"fear\",\n",
        "    \"gratitude\": \"joy\",\n",
        "    \"grief\": \"sadness\",\n",
        "    \"joy\": \"joy\",\n",
        "    \"love\": \"joy\",\n",
        "    \"nervousness\": \"fear\",\n",
        "    \"optimism\": \"anticipation\",\n",
        "    \"pride\": \"joy\",\n",
        "    \"realization\": \"surprise\",\n",
        "    \"relief\": \"joy\",\n",
        "    \"remorse\": \"sadness\",\n",
        "    \"sadness\": \"sadness\",\n",
        "    \"surprise\": \"surprise\",\n",
        "    \"neutral\": \"neutral\",\n",
        "}\n",
        "\n",
        "\n",
        "def plutchik_distance(primary_a: str, primary_b: str) -> float:\n",
        "    \"\"\"\n",
        "    Very simple distance on Plutchik primary categories:\n",
        "    - 0.0 if same category\n",
        "    - 0.5 if one of them is 'neutral'\n",
        "    - 1.0 otherwise\n",
        "    \"\"\"\n",
        "    if primary_a == primary_b:\n",
        "        return 0.0\n",
        "    if \"neutral\" in (primary_a, primary_b):\n",
        "        return 0.5\n",
        "    return 1.0\n",
        "\n",
        "\n",
        "def compute_plutchik_metrics_multilabel(\n",
        "    logits: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    id2label: Dict[int, str],\n",
        "    threshold: float = 0.5,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Multi-label evaluation:\n",
        "    - standard micro F1\n",
        "    - Plutchik \"soft\" F1 that gives partial credit when\n",
        "      predicted emotions fall in the same Plutchik primary category.\n",
        "    \"\"\"\n",
        "    sigmoid = 1.0 / (1.0 + np.exp(-logits))\n",
        "    preds = (sigmoid >= threshold).astype(int)\n",
        "\n",
        "    eps = 1e-8\n",
        "\n",
        "    # Standard micro F1\n",
        "    tp = np.sum((preds == 1) & (labels == 1))\n",
        "    fp = np.sum((preds == 1) & (labels == 0))\n",
        "    fn = np.sum((preds == 0) & (labels == 1))\n",
        "\n",
        "    prec = tp / (tp + fp + eps)\n",
        "    rec = tp / (tp + fn + eps)\n",
        "    f1_micro = 2 * prec * rec / (prec + rec + eps)\n",
        "\n",
        "    # Precompute primary category for each label id\n",
        "    num_labels = len(id2label)\n",
        "    id_to_primary = [\n",
        "        RAW_LABEL_TO_PLUTCHIK.get(id2label[i], \"neutral\") for i in range(num_labels)\n",
        "    ]\n",
        "\n",
        "    soft_tp = 0.0\n",
        "    soft_fp = 0.0\n",
        "    soft_fn = 0.0\n",
        "\n",
        "    # Plutchik-aware soft counts\n",
        "    for i in range(labels.shape[0]):\n",
        "        true_ids = np.where(labels[i] == 1)[0].tolist()\n",
        "        pred_ids = np.where(preds[i] == 1)[0].tolist()\n",
        "\n",
        "        true_set = set(true_ids)\n",
        "        pred_set = set(pred_ids)\n",
        "\n",
        "        for pid in pred_ids:\n",
        "            if pid in true_set:\n",
        "                soft_tp += 1.0\n",
        "            else:\n",
        "                p_primary = id_to_primary[pid]\n",
        "                # Check if there is a true label with the same primary category\n",
        "                if any(\n",
        "                    plutchik_distance(\n",
        "                        p_primary,\n",
        "                        id_to_primary[tid],\n",
        "                    )\n",
        "                    == 0.0\n",
        "                    for tid in true_ids\n",
        "                ):\n",
        "                    soft_tp += 0.5\n",
        "                else:\n",
        "                    soft_fp += 1.0\n",
        "\n",
        "        for tid in true_ids:\n",
        "            if tid not in pred_set:\n",
        "                soft_fn += 1.0\n",
        "\n",
        "    s_prec = soft_tp / (soft_tp + soft_fp + eps)\n",
        "    s_rec = soft_tp / (soft_tp + soft_fn + eps)\n",
        "    plutchik_f1 = 2 * s_prec * s_rec / (s_prec + s_rec + eps)\n",
        "\n",
        "    return {\n",
        "        \"f1_micro\": float(f1_micro),\n",
        "        \"plutchik_f1\": float(plutchik_f1),\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_plutchik_metrics_singlelabel(\n",
        "    logits: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    id2label: Dict[int, str],\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Single-label evaluation (simplified config):\n",
        "    - accuracy\n",
        "    - Plutchik \"soft\" score with partial credit:\n",
        "        1.0  for exact match\n",
        "        0.5  if same primary category\n",
        "        0.25 if one is neutral w.r.t. Plutchik distance\n",
        "    \"\"\"\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = (preds == labels).mean()\n",
        "    soft_score = 0.0\n",
        "    n = len(labels)\n",
        "\n",
        "    num_labels = len(id2label)\n",
        "    id_to_primary = [\n",
        "        RAW_LABEL_TO_PLUTCHIK.get(id2label[i], \"neutral\") for i in range(num_labels)\n",
        "    ]\n",
        "\n",
        "    for p, t in zip(preds, labels):\n",
        "        if p == t:\n",
        "            soft_score += 1.0\n",
        "        else:\n",
        "            primary_p = id_to_primary[p]\n",
        "            primary_t = id_to_primary[t]\n",
        "            d = plutchik_distance(primary_p, primary_t)\n",
        "            if d == 0.0:\n",
        "                soft_score += 0.5\n",
        "            elif d == 0.5:\n",
        "                soft_score += 0.25\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": float(acc),\n",
        "        \"plutchik_soft\": float(soft_score / n),\n",
        "    }\n",
        "\n",
        "\n",
        "def make_compute_metrics_raw(id2label: Dict[int, str]):\n",
        "    \"\"\"\n",
        "    Factory that returns a compute_metrics function for the RAW (multi-label) setting,\n",
        "    ready to be passed to the HF Trainer.\n",
        "    \"\"\"\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        logits = np.array(logits)\n",
        "        labels = np.array(labels)\n",
        "        return compute_plutchik_metrics_multilabel(logits, labels, id2label)\n",
        "\n",
        "    return compute_metrics\n",
        "\n",
        "\n",
        "def make_compute_metrics_simplified(id2label: Dict[int, str]):\n",
        "    \"\"\"\n",
        "    Factory that returns a compute_metrics function for the SIMPLIFIED (single-label) setting.\n",
        "    \"\"\"\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        logits = np.array(logits)\n",
        "        labels = np.array(labels)\n",
        "        return compute_plutchik_metrics_singlelabel(logits, labels, id2label)\n",
        "\n",
        "    return compute_metrics\n",
        "\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Extension of HF Trainer with:\n",
        "    - BCEWithLogitsLoss + pos_weight for multi-label\n",
        "    - CrossEntropyLoss + class weights for single-label\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        is_multilabel: bool,\n",
        "        pos_weight: torch.Tensor = None,\n",
        "        class_weights: torch.Tensor = None,\n",
        "        num_labels: int = None,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.is_multilabel = is_multilabel\n",
        "        self.pos_weight = pos_weight\n",
        "        self.class_weights = class_weights\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        if self.pos_weight is not None:\n",
        "            self.pos_weight = self.pos_weight.to(self.args.device)\n",
        "        if self.class_weights is not None:\n",
        "            self.class_weights = self.class_weights.to(self.args.device)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if self.is_multilabel:\n",
        "            # Multi-label: BCE with logits, optional pos_weight for imbalance\n",
        "            labels = labels.to(logits.dtype)\n",
        "            loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n",
        "            loss = loss_fct(logits, labels)\n",
        "        else:\n",
        "            # Single-label: standard CrossEntropy with optional class weights\n",
        "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH_SrBnETYH7"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# 4. Run a single HPO trial and save results to Drive\n",
        "# ====================================================\n",
        "\n",
        "def save_json(obj, path: str):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "\n",
        "def run_hpo_trial(\n",
        "    base_model_name: str,\n",
        "    mode: str,         # \"raw\" or \"simplified\"\n",
        "    trial_index: int,  # HPO grid index\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    # Disable HPO in MINI mode (safety)\n",
        "    if SMOKE_TEST:\n",
        "        raise RuntimeError(\n",
        "            \"HPO (run_hpo_trial) is disabled in SMOKE_TEST. \"\n",
        "            \"Set SMOKE_TEST=False for full runs.\"\n",
        "        )\n",
        "\n",
        "    assert mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]\n",
        "    assert 0 <= trial_index < len(HPO_GRID)\n",
        "\n",
        "    set_global_seed(SEED)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Load HPO config\n",
        "    # -------------------------------------------------\n",
        "    cfg = HPO_GRID[trial_index]\n",
        "    model_key = base_model_name.replace(\"/\", \"_\")\n",
        "\n",
        "    # effective batch size (prevent OOM on large models)\n",
        "    effective_bs = cfg[\"batch_size\"]\n",
        "    if \"roberta\" in base_model_name.lower():\n",
        "        effective_bs = min(effective_bs, 8)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Prepare trial dirs\n",
        "    # -------------------------------------------------\n",
        "    trial_root = os.path.join(TRIALS_DIR, f\"{model_key}_{mode}\")\n",
        "    os.makedirs(trial_root, exist_ok=True)\n",
        "\n",
        "    trial_dir = os.path.join(trial_root, f\"trial_{trial_index:02d}\")\n",
        "    os.makedirs(trial_dir, exist_ok=True)\n",
        "\n",
        "    metrics_path = os.path.join(trial_dir, \"metrics.json\")\n",
        "    if os.path.exists(metrics_path):\n",
        "        print(f\"[SKIP] Trial already exists, loading metrics from {metrics_path}\")\n",
        "        with open(metrics_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(f\"Running HPO TRIAL for {base_model_name} | mode={mode} | trial={trial_index}\")\n",
        "    print(\"HPO config:\", cfg)\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Load tokenized dataset\n",
        "    # -------------------------------------------------\n",
        "    tokenized_ds, tokenizer, label_info = get_tokenized_dataset_cached(\n",
        "        base_model_name, mode, max_len=cfg[\"max_length\"]\n",
        "    )\n",
        "\n",
        "    num_labels = label_info[\"num_labels\"]\n",
        "    id2label = label_info[\"id2label\"]\n",
        "    label2id = label_info[\"label2id\"]\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Build model config\n",
        "    # -------------------------------------------------\n",
        "    problem_type = (\n",
        "        \"multi_label_classification\" if mode == RAW_CONFIG_NAME\n",
        "        else \"single_label_classification\"\n",
        "    )\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        base_model_name,\n",
        "        num_labels=num_labels,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "        problem_type=problem_type,\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Metrics + loss mode\n",
        "    # -------------------------------------------------\n",
        "    if mode == RAW_CONFIG_NAME:\n",
        "        compute_metrics = make_compute_metrics_raw(id2label)\n",
        "        is_multilabel = True\n",
        "        pos_weight = label_info[\"pos_weight\"]\n",
        "        class_weights = None\n",
        "        best_metric_name = \"plutchik_f1\"\n",
        "    else:\n",
        "        compute_metrics = make_compute_metrics_simplified(id2label)\n",
        "        is_multilabel = False\n",
        "        pos_weight = None\n",
        "        class_weights = label_info[\"class_weights\"]\n",
        "        best_metric_name = \"plutchik_soft\"\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Load pretrained model\n",
        "    # -------------------------------------------------\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        base_model_name,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # TrainingArguments (fully HPO-driven)\n",
        "    # -------------------------------------------------\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=trial_dir,\n",
        "\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "\n",
        "        learning_rate=cfg[\"learning_rate\"],\n",
        "        warmup_ratio=cfg[\"warmup_ratio\"],\n",
        "        weight_decay=cfg[\"weight_decay\"],\n",
        "        num_train_epochs=NUM_EPOCHS_HPO,\n",
        "        lr_scheduler_type=cfg[\"scheduler\"],\n",
        "        optim=cfg[\"optimizer\"],\n",
        "\n",
        "        per_device_train_batch_size=effective_bs,\n",
        "        per_device_eval_batch_size=effective_bs,\n",
        "\n",
        "        logging_steps=200,\n",
        "        report_to=[],\n",
        "        seed=SEED,\n",
        "\n",
        "        fp16=False,\n",
        "        #bf16=torch.cuda.is_available(),   # stable on Ampere GPUs\n",
        "        bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Trainer\n",
        "    # -------------------------------------------------\n",
        "    trainer = WeightedTrainer(\n",
        "        is_multilabel=is_multilabel,\n",
        "        pos_weight=pos_weight,\n",
        "        class_weights=class_weights,\n",
        "        num_labels=num_labels,\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_ds[\"train\"],\n",
        "        eval_dataset=tokenized_ds[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Run training + evaluation\n",
        "    trainer.train()\n",
        "    eval_metrics = trainer.evaluate(tokenized_ds[\"validation\"])\n",
        "    print(\"Eval metrics:\", eval_metrics)\n",
        "\n",
        "    main_metric = eval_metrics.get(best_metric_name)\n",
        "    if main_metric is None:  # backup fallback\n",
        "        main_metric = eval_metrics.get(\"f1_micro\", eval_metrics.get(\"accuracy\", 0.0))\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Save results\n",
        "    # -------------------------------------------------\n",
        "    result = {\n",
        "        \"base_model_name\": base_model_name,\n",
        "        \"mode\": mode,\n",
        "        \"trial_index\": trial_index,\n",
        "        \"hpo_config\": cfg,\n",
        "        \"eval_metrics\": eval_metrics,\n",
        "        \"main_metric_name\": best_metric_name,\n",
        "        \"main_metric_value\": float(main_metric),\n",
        "    }\n",
        "\n",
        "    trainer.save_model(trial_dir)\n",
        "    tokenizer.save_pretrained(trial_dir)\n",
        "    save_json(result, metrics_path)\n",
        "\n",
        "    print(f\"[DONE] Trial metrics saved to {metrics_path}\")\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1XeTlrOTYF6"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# 5. Best trial selection + quantization + export\n",
        "# ====================================================\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "def load_all_trials(base_model_name: str, mode: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Loads all HPO trial results (metrics.json) for a given base model and mode.\n",
        "    \"\"\"\n",
        "    model_key = base_model_name.replace(\"/\", \"_\")\n",
        "    trial_root = os.path.join(TRIALS_DIR, f\"{model_key}_{mode}\")\n",
        "\n",
        "    if not os.path.exists(trial_root):\n",
        "        print(\"No trials directory:\", trial_root)\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    for trial_dir in sorted(glob(os.path.join(trial_root, \"trial_*\"))):\n",
        "        metrics_path = os.path.join(trial_dir, \"metrics.json\")\n",
        "        if not os.path.exists(metrics_path):\n",
        "            continue\n",
        "        with open(metrics_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            res = json.load(f)\n",
        "        res[\"trial_dir\"] = trial_dir\n",
        "        results.append(res)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def select_best_trial(\n",
        "    base_model_name: str,\n",
        "    mode: str,\n",
        "    metric_name: str = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Selects the best HPO trial based on the target metric.\n",
        "    Uses Plutchik F1 (RAW) or Plutchik soft score (SIMPLIFIED) by default.\n",
        "    \"\"\"\n",
        "    results = load_all_trials(base_model_name, mode)\n",
        "    if not results:\n",
        "        raise ValueError(\"No trial results found. Run run_hpo_trial first.\")\n",
        "\n",
        "    if metric_name is None:\n",
        "        metric_name = \"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"\n",
        "\n",
        "    best = None\n",
        "    best_value = -1e9\n",
        "\n",
        "    for r in results:\n",
        "        mv = r[\"eval_metrics\"].get(metric_name)\n",
        "\n",
        "        if mv is None:\n",
        "            mv = r[\"eval_metrics\"].get(\"plutchik_f1\", r[\"eval_metrics\"].get(\"plutchik_soft\"))\n",
        "\n",
        "        if mv is None:\n",
        "            continue\n",
        "\n",
        "        if mv > best_value:\n",
        "            best_value = mv\n",
        "            best = r\n",
        "\n",
        "    if best is None:\n",
        "        raise ValueError(f\"No trial contains metric '{metric_name}' or fallback metrics.\")\n",
        "\n",
        "    print(f\"Best trial for {base_model_name} | mode={mode}\")\n",
        "    print(f\"  trial_index = {best['trial_index']}\")\n",
        "    print(f\"  {metric_name} (fallback) = {best_value:.4f}\")\n",
        "    print(f\"  trial_dir = {best['trial_dir']}\")\n",
        "\n",
        "    return best\n",
        "\n",
        "\n",
        "def quantize_best_trial(\n",
        "    base_model_name: str,\n",
        "    mode: str,\n",
        "    metric_name: str = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Loads the best-performing model, attempts dynamic quantization,\n",
        "    and exports the quantized model (or FP32 fallback).\n",
        "    \"\"\"\n",
        "\n",
        "    if SMOKE_TEST:\n",
        "        raise RuntimeError(\n",
        "            \"Quantization/export is disabled in SMOKE_TEST. \"\n",
        "            \"Set SMOKE_TEST=False and rerun HPO in FULL mode.\"\n",
        "        )\n",
        "\n",
        "    best = select_best_trial(base_model_name, mode, metric_name)\n",
        "    trial_dir = best[\"trial_dir\"]\n",
        "\n",
        "    print(\"\\nExporting (with quantization attempt) from:\", trial_dir)\n",
        "\n",
        "    # Load model on CPU for quantization\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(trial_dir).cpu()\n",
        "    model.eval()\n",
        "\n",
        "    model_key = base_model_name.replace(\"/\", \"_\")\n",
        "    out_dir = os.path.join(\n",
        "        FINAL_DIR,\n",
        "        f\"{model_key}_{mode}_quantized\",\n",
        "    )\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    quantized = False\n",
        "\n",
        "    # Attempt PyTorch dynamic quantization\n",
        "    try:\n",
        "        try:\n",
        "            from torch.ao.quantization import quantize_dynamic\n",
        "        except ImportError:\n",
        "            from torch.quantization import quantize_dynamic\n",
        "\n",
        "        qmodel = quantize_dynamic(\n",
        "            model,\n",
        "            {nn.Linear},\n",
        "            dtype=torch.qint8,\n",
        "        )\n",
        "        qmodel.save_pretrained(out_dir)\n",
        "        quantized = True\n",
        "        print(\"Dynamic quantization successful.\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Quantization failed, exporting FP32 instead:\\n\", e)\n",
        "        model.save_pretrained(out_dir)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(trial_dir, use_fast=True)\n",
        "    tokenizer.save_pretrained(out_dir)\n",
        "\n",
        "    summary = {\n",
        "        \"base_model_name\": base_model_name,\n",
        "        \"mode\": mode,\n",
        "        \"metric_name\": metric_name or (\"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"),\n",
        "        \"metric_value\": best[\"eval_metrics\"].get(\n",
        "            metric_name or (\"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\")\n",
        "        ),\n",
        "        \"best_trial_index\": best[\"trial_index\"],\n",
        "        \"best_trial_dir\": trial_dir,\n",
        "        \"quantized_dir\": out_dir,\n",
        "        \"actually_quantized\": quantized,\n",
        "    }\n",
        "\n",
        "    save_json(summary, os.path.join(out_dir, \"quantized_summary.json\"))\n",
        "    print(\"Final model saved to:\", out_dir, \"| quantized =\", quantized)\n",
        "\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96BqLZ7Rd0p6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehLaOrD-h7_n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw6KPiW7h-l0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hicCrAKth7E5"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HPO Launcher (improved)\n",
        "# ============================================================\n",
        "\n",
        "if not SMOKE_TEST:\n",
        "  for model_cfg in BASE_MODELS:\n",
        "      if model_cfg[\"hpo\"]:\n",
        "          model_name = model_cfg[\"name\"]\n",
        "          mode = model_cfg[\"hpo_mode\"]\n",
        "\n",
        "          print(\"\\n\" + \"#\"*80)\n",
        "          print(f\"RUNNING HPO FOR MODEL: {model_name} | MODE: {mode}\")\n",
        "          print(\"#\"*80)\n",
        "\n",
        "          for trial_idx in range(len(HPO_GRID)):\n",
        "              print(f\"\\nRunning HPO trial {trial_idx+1} / {len(HPO_GRID)}\")\n",
        "              run_hpo_trial(model_name, mode, trial_idx)\n",
        "\n",
        "          print(\"\\n=== HPO finished ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AawNlxlfxncS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X82FdBCBxnZt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXBdyVUOxnW7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk6CJZBCd0nv"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TRAINING Launcher (uses best HPO config)\n",
        "# ============================================================\n",
        "if not SMOKE_TEST:\n",
        "  for model_cfg in BASE_MODELS:\n",
        "      if model_cfg[\"train\"]:\n",
        "          model_name = model_cfg[\"name\"]\n",
        "          mode = model_cfg[\"train_mode\"]\n",
        "\n",
        "          print(\"\\n\" + \"#\"*80)\n",
        "          print(f\"RUNNING FINAL TRAINING FOR MODEL: {model_name} | MODE: {mode}\")\n",
        "          print(\"#\"*80)\n",
        "\n",
        "          # =========================================\n",
        "          # Load best HPO trial\n",
        "          # =========================================\n",
        "          best = select_best_trial(model_name, RAW_CONFIG_NAME)\n",
        "\n",
        "          best_trial_idx = best[\"trial_index\"]\n",
        "          best_cfg = best[\"hpo_config\"]\n",
        "\n",
        "          print(f\"Using best trial index: {best_trial_idx}\")\n",
        "          print(\"Best HPO config:\", best_cfg)\n",
        "\n",
        "          # =========================================\n",
        "          # Load dataset (tokenized)\n",
        "          # =========================================\n",
        "          tokenized_ds, tokenizer, label_info = get_tokenized_dataset_cached(\n",
        "              base_model_name=model_name,\n",
        "              mode=mode,\n",
        "              max_len=best_cfg[\"max_length\"],\n",
        "          )\n",
        "\n",
        "          num_labels = label_info[\"num_labels\"]\n",
        "          id2label = label_info[\"id2label\"]\n",
        "          label2id = label_info[\"label2id\"]\n",
        "\n",
        "          problem_type = (\n",
        "              \"multi_label_classification\" if mode == RAW_CONFIG_NAME\n",
        "              else \"single_label_classification\"\n",
        "          )\n",
        "\n",
        "          # =========================================\n",
        "          # Build config + model\n",
        "          # =========================================\n",
        "          config = AutoConfig.from_pretrained(\n",
        "              model_name,\n",
        "              num_labels=num_labels,\n",
        "              id2label=id2label,\n",
        "              label2id=label2id,\n",
        "              problem_type=problem_type,\n",
        "          )\n",
        "\n",
        "          model = AutoModelForSequenceClassification.from_pretrained(\n",
        "              model_name,\n",
        "              config=config,\n",
        "          )\n",
        "\n",
        "          # =========================================\n",
        "          # Loss + metrics\n",
        "          # =========================================\n",
        "          if mode == RAW_CONFIG_NAME:\n",
        "              compute_metrics = make_compute_metrics_raw(id2label)\n",
        "              is_multilabel = True\n",
        "              pos_weight = label_info[\"pos_weight\"]\n",
        "              class_weights = None\n",
        "              best_metric_name = \"plutchik_f1\"\n",
        "          else:\n",
        "              compute_metrics = make_compute_metrics_simplified(id2label)\n",
        "              is_multilabel = False\n",
        "              pos_weight = None\n",
        "              class_weights = label_info[\"class_weights\"]\n",
        "              best_metric_name = \"plutchik_soft\"\n",
        "\n",
        "          # =========================================\n",
        "          # TrainingArguments using best HPO config\n",
        "          # =========================================\n",
        "          out_dir = os.path.join(\n",
        "              FINAL_DIR,\n",
        "              f\"{model_name.replace('/', '_')}_{mode}_final_trained\"\n",
        "          )\n",
        "          os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "          effective_bs = best_cfg[\"batch_size\"]\n",
        "          if \"roberta\" in model_name.lower():\n",
        "              effective_bs = min(effective_bs, 8)\n",
        "\n",
        "          training_args = TrainingArguments(\n",
        "              output_dir=out_dir,\n",
        "              learning_rate=best_cfg[\"learning_rate\"],\n",
        "              warmup_ratio=best_cfg[\"warmup_ratio\"],\n",
        "              weight_decay=best_cfg[\"weight_decay\"],\n",
        "              lr_scheduler_type=best_cfg[\"scheduler\"],\n",
        "              optim=best_cfg[\"optimizer\"],\n",
        "              num_train_epochs=NUM_EPOCHS,\n",
        "\n",
        "              per_device_train_batch_size=effective_bs,\n",
        "              per_device_eval_batch_size=effective_bs,\n",
        "              #per_device_train_batch_size=best_cfg[\"batch_size\"],\n",
        "              #per_device_eval_batch_size=best_cfg[\"batch_size\"],\n",
        "\n",
        "              eval_strategy=\"epoch\",\n",
        "              save_strategy=\"epoch\",\n",
        "              save_total_limit=2,\n",
        "              load_best_model_at_end=True,\n",
        "              report_to=[],\n",
        "              seed=SEED,\n",
        "              fp16=False,\n",
        "              #bf16=torch.cuda.is_available(),\n",
        "              bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
        "          )\n",
        "\n",
        "          # =========================================\n",
        "          # Trainer\n",
        "          # =========================================\n",
        "          trainer = WeightedTrainer(\n",
        "              is_multilabel=is_multilabel,\n",
        "              pos_weight=pos_weight,\n",
        "              class_weights=class_weights,\n",
        "              num_labels=num_labels,\n",
        "              model=model,\n",
        "              args=training_args,\n",
        "              train_dataset=tokenized_ds[\"train\"],\n",
        "              eval_dataset=tokenized_ds[\"validation\"],\n",
        "              tokenizer=tokenizer,\n",
        "              compute_metrics=compute_metrics,\n",
        "          )\n",
        "\n",
        "          # =========================================\n",
        "          # TRAIN + EVALUATE\n",
        "          # =========================================\n",
        "          trainer.train()\n",
        "          final_metrics = trainer.evaluate(tokenized_ds[\"validation\"])\n",
        "\n",
        "          print(\"\\nFinal evaluation metrics:\")\n",
        "          print(final_metrics)\n",
        "\n",
        "          # Save summary\n",
        "          summary_path = os.path.join(out_dir, \"training_summary.json\")\n",
        "          save_json({\n",
        "              \"model_name\": model_name,\n",
        "              \"mode\": mode,\n",
        "              \"best_trial_index\": best_trial_idx,\n",
        "              \"best_hpo_config\": best_cfg,\n",
        "              \"final_metrics\": final_metrics,\n",
        "          }, summary_path)\n",
        "\n",
        "          print(f\"\\nSaved final trained model + summary to {out_dir}\")\n",
        "          print(\"#\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvk478rFqJ1N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69TwmlZbqJyg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqE8PcbAqJwP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujOduuDZans8"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# QUANTIZATION + FINAL EVALUATION Launcher (SAFE MODE)\n",
        "# ============================================================\n",
        "\n",
        "#assert not SMOKE_TEST, \"❌ Quantization cannot run in SMOKE_TEST=True!\"\n",
        "if not RUN_QUANTIZATION:\n",
        "    print(\"⏭ Quantization skipped. Set RUN_QUANTIZATION=True to run.\")\n",
        "else:\n",
        "    print(\"🚀 Starting QUANTIZATION + FINAL EVALUATION pipeline...\")\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "if RUN_QUANTIZATION:\n",
        "    for model_cfg in BASE_MODELS:\n",
        "      for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "\n",
        "        if not model_cfg[\"trained\"]:\n",
        "            continue   # skip models not trained\n",
        "\n",
        "        model_name = model_cfg[\"name\"]\n",
        "\n",
        "        # You can change this to run both versions:\n",
        "        # for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "        #mode = RAW_CONFIG_NAME\n",
        "\n",
        "        print(\"\\n\" + \"#\"*80)\n",
        "        print(f\"QUANTIZATION + FINAL EVALUATION for MODEL: {model_name} | MODE: {mode}\")\n",
        "        print(\"#\"*80)\n",
        "\n",
        "        # ===============================\n",
        "        # Check trained model directory\n",
        "        # ===============================\n",
        "        trained_dir = os.path.join(\n",
        "            FINAL_DIR,\n",
        "            f\"{model_name.replace('/', '_')}_{mode}_final_trained\",\n",
        "        )\n",
        "        if not os.path.exists(trained_dir):\n",
        "            print(f\"❌ Trained model not found at: {trained_dir}\")\n",
        "            print(\"Skipping this model...\\n\")\n",
        "            continue\n",
        "\n",
        "        # Output directory\n",
        "        quantized_out = os.path.join(\n",
        "            FINAL_DIR,\n",
        "            f\"{model_name.replace('/', '_')}_{mode}_quantized_final\",\n",
        "        )\n",
        "        os.makedirs(quantized_out, exist_ok=True)\n",
        "\n",
        "        # ===============================\n",
        "        # Load dataset (cached)\n",
        "        # ===============================\n",
        "        tokenized_ds, tokenizer, label_info = get_tokenized_dataset_cached(\n",
        "            base_model_name=model_name,\n",
        "            mode=mode,\n",
        "            max_len=MAX_LENGTH,\n",
        "        )\n",
        "\n",
        "        num_labels = label_info[\"num_labels\"]\n",
        "        id2label = label_info[\"id2label\"]\n",
        "        label2id = label_info[\"label2id\"]\n",
        "\n",
        "        # ===============================\n",
        "        # Build model config\n",
        "        # ===============================\n",
        "        problem_type = (\n",
        "            \"multi_label_classification\" if mode == RAW_CONFIG_NAME\n",
        "            else \"single_label_classification\"\n",
        "        )\n",
        "\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            trained_dir,\n",
        "            num_labels=num_labels,\n",
        "            id2label=id2label,\n",
        "            label2id=label2id,\n",
        "            problem_type=problem_type,\n",
        "        )\n",
        "\n",
        "        # ===============================\n",
        "        # Load trained model (CPU)\n",
        "        # ===============================\n",
        "        print(\"Loading trained model...\")\n",
        "        model_cpu = AutoModelForSequenceClassification.from_pretrained(\n",
        "            trained_dir,\n",
        "            config=config,\n",
        "        ).cpu()\n",
        "        model_cpu.eval()\n",
        "\n",
        "        # ===============================\n",
        "        # Attempt dynamic quantization\n",
        "        # ===============================\n",
        "        print(\"Attempting dynamic quantization...\")\n",
        "        quantized_success = False\n",
        "\n",
        "        try:\n",
        "            try:\n",
        "                from torch.ao.quantization import quantize_dynamic\n",
        "            except ImportError:\n",
        "                from torch.quantization import quantize_dynamic\n",
        "\n",
        "            qmodel = quantize_dynamic(\n",
        "                model_cpu,\n",
        "                {torch.nn.Linear},\n",
        "                dtype=torch.qint8,\n",
        "            )\n",
        "            qmodel.save_pretrained(quantized_out)\n",
        "            quantized_success = True\n",
        "            print(\"✔ Dynamic quantization successful!\")\n",
        "        except Exception as e:\n",
        "            print(\"⚠ Quantization failed, exporting FP32. Error:\", e)\n",
        "            model_cpu.save_pretrained(quantized_out)\n",
        "\n",
        "        tokenizer.save_pretrained(quantized_out)\n",
        "\n",
        "        # ===============================\n",
        "        # Final evaluation\n",
        "        # ===============================\n",
        "        print(\"\\nRunning evaluation of quantized model...\")\n",
        "        eval_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            quantized_out,\n",
        "            config=config,\n",
        "        ).to(DEVICE)\n",
        "        eval_model.eval()\n",
        "\n",
        "        if mode == RAW_CONFIG_NAME:\n",
        "            compute_metrics = make_compute_metrics_raw(id2label)\n",
        "            is_multilabel = True\n",
        "            pos_weight = label_info[\"pos_weight\"]\n",
        "            class_weights = None\n",
        "        else:\n",
        "            compute_metrics = make_compute_metrics_simplified(id2label)\n",
        "            is_multilabel = False\n",
        "            pos_weight = None\n",
        "            class_weights = label_info[\"class_weights\"]\n",
        "\n",
        "        eval_args = TrainingArguments(\n",
        "            output_dir=os.path.join(quantized_out, \"eval_tmp\"),\n",
        "            per_device_eval_batch_size=8,\n",
        "            report_to=[],\n",
        "        )\n",
        "\n",
        "        eval_trainer = WeightedTrainer(\n",
        "            is_multilabel=is_multilabel,\n",
        "            pos_weight=pos_weight,\n",
        "            class_weights=class_weights,\n",
        "            num_labels=num_labels,\n",
        "            model=eval_model,\n",
        "            args=eval_args,\n",
        "            eval_dataset=tokenized_ds[\"validation\"],\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        quant_metrics = eval_trainer.evaluate()\n",
        "\n",
        "        print(\"\\nFinal quantized model metrics:\")\n",
        "        print(quant_metrics)\n",
        "\n",
        "        # ===============================\n",
        "        # Save summary\n",
        "        # ===============================\n",
        "        summary = {\n",
        "            \"model_name\": model_name,\n",
        "            \"mode\": mode,\n",
        "            \"quantized_dir\": quantized_out,\n",
        "            \"quantized_success\": quantized_success,\n",
        "            \"quantized_metrics\": quant_metrics,\n",
        "        }\n",
        "\n",
        "        save_json(summary, os.path.join(quantized_out, \"quantization_summary.json\"))\n",
        "\n",
        "        print(f\"\\n✔ Saved quantized model + summary to {quantized_out}\")\n",
        "        print(\"#\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHkdtD3LanrN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cw3wI8manmm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lSsELzKTYBi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShXpyLz1TX_S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgeWKYU9Y9tF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txt5B6sOY9q_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_yGE9ciY9op"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# SMOKE TEST (NO TOKENIZATION, NO DATASET DOWNLOAD)\n",
        "# =========================================================\n",
        "# Uses only cached tokenized datasets.\n",
        "# Runs a 1-epoch training for each (model × mode).\n",
        "# =========================================================\n",
        "\n",
        "if SMOKE_TEST:\n",
        "  SMOKE_EPOCHS = 1\n",
        "  smoke_results = {}\n",
        "\n",
        "  for model_cfg in BASE_MODELS:\n",
        "      model_name = model_cfg[\"name\"]\n",
        "      for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "\n",
        "          print(\"\\n\" + \"=\"*80)\n",
        "          print(f\"SMOKE TEST: model={model_name} | mode={mode}\")\n",
        "          print(\"=\"*80)\n",
        "\n",
        "          # ------------------------------------------\n",
        "          # Load *cached* tokenized dataset only\n",
        "          # ------------------------------------------\n",
        "          tokenized_ds, tokenizer, label_info = get_tokenized_dataset_cached(\n",
        "              base_model_name=model_name,\n",
        "              mode=mode,\n",
        "              max_len=MAX_LENGTH,   # same as your earlier tokenization\n",
        "          )\n",
        "\n",
        "          num_labels = label_info[\"num_labels\"]\n",
        "          id2label = label_info[\"id2label\"]\n",
        "          label2id = label_info[\"label2id\"]\n",
        "\n",
        "          # ------------------------------------------\n",
        "          # Build config\n",
        "          # ------------------------------------------\n",
        "          problem_type = (\n",
        "              \"multi_label_classification\" if mode == RAW_CONFIG_NAME\n",
        "              else \"single_label_classification\"\n",
        "          )\n",
        "\n",
        "          #config = AutoConfig.from_pretrained(\n",
        "          #    model_name,\n",
        "          #    num_labels=num_labels,\n",
        "          #    id2label=id2label,\n",
        "          #    label2id=label2id,\n",
        "          #    problem_type=problem_type,\n",
        "          #)\n",
        "          config = AutoConfig.from_pretrained(model_name)\n",
        "          config.num_labels = num_labels\n",
        "          config.id2label = id2label\n",
        "          config.label2id = label2id\n",
        "          config.problem_type = problem_type\n",
        "\n",
        "\n",
        "\n",
        "          # ------------------------------------------\n",
        "          # Metrics + loss flags\n",
        "          # ------------------------------------------\n",
        "          if mode == RAW_CONFIG_NAME:\n",
        "              compute_metrics = make_compute_metrics_raw(id2label)\n",
        "              is_multilabel = True\n",
        "              pos_weight = label_info[\"pos_weight\"]\n",
        "              class_weights = None\n",
        "          else:\n",
        "              compute_metrics = make_compute_metrics_simplified(id2label)\n",
        "              is_multilabel = False\n",
        "              pos_weight = None\n",
        "              class_weights = label_info[\"class_weights\"]\n",
        "\n",
        "          # ------------------------------------------\n",
        "          # Load pretrained model\n",
        "          # ------------------------------------------\n",
        "          model = AutoModelForSequenceClassification.from_pretrained(\n",
        "              model_name,\n",
        "              config=config,\n",
        "          )\n",
        "\n",
        "          # ------------------------------------------\n",
        "          # Minimal training args\n",
        "          # ------------------------------------------\n",
        "          training_args = TrainingArguments(\n",
        "              output_dir=f\"/content/smoke_tmp/{model_name.replace('/', '_')}_{mode}\",\n",
        "              per_device_train_batch_size=4,\n",
        "              per_device_eval_batch_size=4,\n",
        "              num_train_epochs=SMOKE_EPOCHS,\n",
        "\n",
        "              eval_strategy=\"epoch\",\n",
        "              save_strategy=\"no\",\n",
        "              report_to=[],\n",
        "              seed=SEED,\n",
        "\n",
        "              fp16=False,\n",
        "              #bf16=torch.cuda.is_available(),\n",
        "              bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
        "          )\n",
        "\n",
        "          # ------------------------------------------\n",
        "          # Trainer\n",
        "          # ------------------------------------------\n",
        "          trainer = WeightedTrainer(\n",
        "              is_multilabel=is_multilabel,\n",
        "              pos_weight=pos_weight,\n",
        "              class_weights=class_weights,\n",
        "              num_labels=num_labels,\n",
        "              model=model,\n",
        "              args=training_args,\n",
        "              train_dataset=tokenized_ds[\"train\"],\n",
        "              eval_dataset=tokenized_ds[\"validation\"],\n",
        "              tokenizer=tokenizer,\n",
        "              compute_metrics=compute_metrics,\n",
        "          )\n",
        "\n",
        "          # ------------------------------------------\n",
        "          # TRAIN + EVAL\n",
        "          # ------------------------------------------\n",
        "          trainer.train()\n",
        "          metrics = trainer.evaluate(tokenized_ds[\"validation\"])\n",
        "\n",
        "          smoke_results[(model_name, mode)] = metrics\n",
        "          print(f\"RESULT ({model_name}, {mode}):\")\n",
        "          print(metrics)\n",
        "          print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyKhwaOVlZc_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59OjhWFKlZay"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il-xUNQhlZYV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkcfThWWl2Li"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O55R2yCl2Iw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6lbB2dvl2GF"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# UNIVERSAL EXPERIMENT DASHBOARD\n",
        "# Added:\n",
        "#   ✔ Summary table (Pandas dataframe)\n",
        "#   ✔ Saving all plots to Drive\n",
        "# ============================================================\n",
        "if GENERATE_FINAL_REPORT:\n",
        "\n",
        "  import json\n",
        "  import matplotlib.pyplot as plt\n",
        "  import pandas as pd\n",
        "  from glob import glob\n",
        "\n",
        "  PLOTS_DIR = os.path.join(FINAL_DIR, \"plots\")\n",
        "  os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "  def savefig(name):\n",
        "      \"\"\"Helper to save current plot to Drive.\"\"\"\n",
        "      path = os.path.join(PLOTS_DIR, name)\n",
        "      plt.savefig(path, bbox_inches=\"tight\", dpi=200)\n",
        "      print(f\"📁 Saved plot: {path}\")\n",
        "\n",
        "\n",
        "  # ------------------------------------------------------------\n",
        "  # Helpers\n",
        "  # ------------------------------------------------------------\n",
        "  def try_load_json(path):\n",
        "      if not os.path.exists(path):\n",
        "          return None\n",
        "      with open(path, \"r\") as f:\n",
        "          return json.load(f)\n",
        "\n",
        "  def find_hpo_results():\n",
        "      results = {}\n",
        "      for model_cfg in BASE_MODELS:\n",
        "          name = model_cfg[\"name\"]\n",
        "          key = name.replace(\"/\", \"_\")\n",
        "          for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "              trial_root = os.path.join(TRIALS_DIR, f\"{key}_{mode}\")\n",
        "              if not os.path.exists(trial_root):\n",
        "                  continue\n",
        "              trial_jsons = sorted(glob(os.path.join(trial_root, \"trial_*/metrics.json\")))\n",
        "              trials = [try_load_json(p) for p in trial_jsons if try_load_json(p)]\n",
        "              if trials:\n",
        "                  results[(name, mode)] = trials\n",
        "      return results\n",
        "\n",
        "  def find_trained_models():\n",
        "      trained = []\n",
        "      for model_cfg in BASE_MODELS:\n",
        "          name = model_cfg[\"name\"]\n",
        "          key = name.replace(\"/\", \"_\")\n",
        "          for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "              p = os.path.join(FINAL_DIR, f\"{key}_{mode}_final_trained\")\n",
        "              if os.path.exists(p):\n",
        "                  trained.append((name, mode, p))\n",
        "      return trained\n",
        "\n",
        "  def find_quantized_models():\n",
        "      quant = []\n",
        "      for model_cfg in BASE_MODELS:\n",
        "          name = model_cfg[\"name\"]\n",
        "          key = name.replace(\"/\", \"_\")\n",
        "          for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "              p = os.path.join(FINAL_DIR, f\"{key}_{mode}_quantized_final\")\n",
        "              if os.path.exists(p):\n",
        "                  summary = try_load_json(os.path.join(p, \"quantization_summary.json\"))\n",
        "                  quant.append((name, mode, p, summary))\n",
        "      return quant\n",
        "\n",
        "\n",
        "  # ============================================================\n",
        "  # 1) PLOT HPO RESULTS (AND SAVE)\n",
        "  # ============================================================\n",
        "\n",
        "  hpo_results = find_hpo_results()\n",
        "\n",
        "  if hpo_results:\n",
        "      print(\"📊 Plotting HPO results...\")\n",
        "\n",
        "      plt.figure(figsize=(12, 6))\n",
        "\n",
        "      for (model_name, mode), trials in hpo_results.items():\n",
        "          metric = \"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"\n",
        "          xs = [t[\"trial_index\"] for t in trials]\n",
        "          ys = [t[\"eval_metrics\"].get(metric, 0.0) for t in trials]\n",
        "          plt.plot(xs, ys, marker=\"o\", label=f\"{model_name} | {mode}\")\n",
        "\n",
        "      plt.title(\"HPO Results Across Models\")\n",
        "      plt.xlabel(\"Trial Index\")\n",
        "      plt.ylabel(\"Score\")\n",
        "      plt.grid(True)\n",
        "      plt.legend()\n",
        "      savefig(\"hpo_results.png\")\n",
        "      plt.show()\n",
        "  else:\n",
        "      print(\"⚠ No HPO results found.\")\n",
        "\n",
        "\n",
        "  # ============================================================\n",
        "  # 2) PLOT TRAINING CURVES (AND SAVE)\n",
        "  # ============================================================\n",
        "\n",
        "  trained_models = find_trained_models()\n",
        "\n",
        "  for (model_name, mode, path) in trained_models:\n",
        "      print(f\"\\n📈 Plotting training curves for {model_name} | {mode}\")\n",
        "\n",
        "      state_path = os.path.join(path, \"trainer_state.json\")\n",
        "      state = try_load_json(state_path)\n",
        "\n",
        "      if not state:\n",
        "          print(\"  ⚠ trainer_state.json missing\")\n",
        "          continue\n",
        "\n",
        "      logs = state.get(\"log_history\", [])\n",
        "      train_loss = [l[\"loss\"] for l in logs if \"loss\" in l]\n",
        "      eval_loss = [l[\"eval_loss\"] for l in logs if \"eval_loss\" in l]\n",
        "\n",
        "      # Training Loss\n",
        "      if train_loss:\n",
        "          plt.figure(figsize=(10,4))\n",
        "          plt.plot(train_loss)\n",
        "          plt.title(f\"Training Loss — {model_name} ({mode})\")\n",
        "          plt.xlabel(\"Step\")\n",
        "          plt.ylabel(\"Loss\")\n",
        "          plt.grid(True)\n",
        "          name = f\"training_loss_{model_name.replace('/', '_')}_{mode}.png\"\n",
        "          savefig(name)\n",
        "          plt.show()\n",
        "\n",
        "      # Eval Loss\n",
        "      if eval_loss:\n",
        "          plt.figure(figsize=(10,4))\n",
        "          plt.plot(eval_loss, marker=\"o\")\n",
        "          plt.title(f\"Eval Loss — {model_name} ({mode})\")\n",
        "          plt.xlabel(\"Epoch\")\n",
        "          plt.ylabel(\"Eval Loss\")\n",
        "          plt.grid(True)\n",
        "          name = f\"eval_loss_{model_name.replace('/', '_')}_{mode}.png\"\n",
        "          savefig(name)\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "  # ============================================================\n",
        "  # 3) FULL vs QUANTIZED PERFORMANCE (AND SAVE)\n",
        "  # ============================================================\n",
        "\n",
        "  quantized_models = find_quantized_models()\n",
        "\n",
        "  for (model_name, mode, p, summary) in quantized_models:\n",
        "      if not summary:\n",
        "          continue\n",
        "\n",
        "      metric = \"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"\n",
        "      #q_val = summary[\"quantized_metrics\"].get(f\"eval_{metric}\")\n",
        "      metric_key = f\"eval_{metric}\"\n",
        "      q_val = summary[\"quantized_metrics\"].get(metric_key)\n",
        "\n",
        "\n",
        "      # Load full model metric\n",
        "      key = model_name.replace(\"/\", \"_\")\n",
        "      full_path = os.path.join(FINAL_DIR, f\"{key}_{mode}_final_trained\")\n",
        "      trainer_state = try_load_json(os.path.join(full_path, \"trainer_state.json\"))\n",
        "\n",
        "      full_val = None\n",
        "      if trainer_state:\n",
        "          eval_logs = [l for l in trainer_state[\"log_history\"] if f\"eval_{metric}\" in l]\n",
        "          if eval_logs:\n",
        "              full_val = eval_logs[-1][f\"eval_{metric}\"]\n",
        "\n",
        "      if full_val is None or q_val is None:\n",
        "          print(f\"⚠ Missing full/quantized metrics for {model_name} ({mode})\")\n",
        "          continue\n",
        "\n",
        "      plt.figure(figsize=(6,4))\n",
        "      plt.bar([\"Full\", \"Quantized\"], [full_val, q_val])\n",
        "      plt.title(f\"{metric}: Full vs Quantized — {model_name} ({mode})\")\n",
        "      plt.ylabel(metric)\n",
        "      plt.grid(axis=\"y\")\n",
        "      name = f\"full_vs_quant_{model_name.replace('/', '_')}_{mode}.png\"\n",
        "      savefig(name)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "  # ============================================================\n",
        "  # 4) SUMMARY TABLE\n",
        "  # ============================================================\n",
        "\n",
        "  rows = []\n",
        "  for (model_name, mode), hpo_trials in hpo_results.items():\n",
        "      # best trial\n",
        "      metric = \"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"\n",
        "      best_trial = max(hpo_trials, key=lambda t: t[\"eval_metrics\"].get(metric, -1))\n",
        "\n",
        "      # trained?\n",
        "      trained_path = os.path.join(\n",
        "          FINAL_DIR,\n",
        "          f\"{model_name.replace('/', '_')}_{mode}_final_trained\"\n",
        "      )\n",
        "      is_trained = os.path.exists(trained_path)\n",
        "\n",
        "      # quantized?\n",
        "      quant_path = os.path.join(\n",
        "          FINAL_DIR,\n",
        "          f\"{model_name.replace('/', '_')}_{mode}_quantized_final\"\n",
        "      )\n",
        "      quant_summary = try_load_json(os.path.join(quant_path, \"quantization_summary.json\"))\n",
        "\n",
        "      rows.append({\n",
        "          \"Model\": model_name,\n",
        "          \"Mode\": mode,\n",
        "          \"Best HPO Trial\": best_trial[\"trial_index\"],\n",
        "          \"Best HPO Score\": best_trial[\"eval_metrics\"].get(metric),\n",
        "          \"Trained?\": is_trained,\n",
        "          \"Trained Path\": trained_path if is_trained else None,\n",
        "          \"Quantized?\": quant_summary is not None,\n",
        "          \"Quantized Score\": quant_summary[\"quantized_metrics\"].get(f\"eval_{metric}\")\n",
        "                    if quant_summary else None,\n",
        "          \"Quantized Path\": quant_path if quant_summary else None,\n",
        "      })\n",
        "\n",
        "  df_summary = pd.DataFrame(rows)\n",
        "  print(\"\\n📘 EXPERIMENT SUMMARY TABLE:\")\n",
        "  display(df_summary)\n",
        "\n",
        "  # Save summary table\n",
        "  df_summary.to_csv(os.path.join(PLOTS_DIR, \"experiment_summary.csv\"), index=False)\n",
        "  print(f\"📁 Saved summary table to: {os.path.join(PLOTS_DIR, 'experiment_summary.csv')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPhk84rOFTJc"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FINAL REPORT GENERATOR → PDF\n",
        "# ============================================================\n",
        "# Output:\n",
        "#   /content/drive/MyDrive/emotion_project/final_report/final_report.pdf\n",
        "# ============================================================\n",
        "\n",
        "if GENERATE_FINAL_REPORT:\n",
        "  !pip install -q reportlab\n",
        "\n",
        "  import os\n",
        "  import json\n",
        "  import datetime\n",
        "  from glob import glob\n",
        "\n",
        "  import pandas as pd\n",
        "  from reportlab.lib.pagesizes import A4\n",
        "  from reportlab.platypus import (\n",
        "      SimpleDocTemplate,\n",
        "      Paragraph,\n",
        "      Spacer,\n",
        "      Table,\n",
        "      TableStyle,\n",
        "      PageBreak,\n",
        "  )\n",
        "  from reportlab.lib.styles import getSampleStyleSheet\n",
        "  from reportlab.lib import colors\n",
        "\n",
        "  # -----------------------------\n",
        "  # Paths\n",
        "  # -----------------------------\n",
        "  REPORT_DIR = os.path.join(PROJECT_ROOT, \"final_report\")\n",
        "  os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "  REPORT_PDF = os.path.join(REPORT_DIR, \"final_report.pdf\")\n",
        "\n",
        "  print(\"Report directory:\", REPORT_DIR)\n",
        "  print(\"Report PDF will be saved to:\", REPORT_PDF)\n",
        "\n",
        "\n",
        "  # -----------------------------\n",
        "  # Helper functions (reuse logic from dashboard)\n",
        "  # -----------------------------\n",
        "  def try_load_json(path):\n",
        "      if not os.path.exists(path):\n",
        "          return None\n",
        "      with open(path, \"r\") as f:\n",
        "          return json.load(f)\n",
        "\n",
        "\n",
        "  def find_hpo_results():\n",
        "      results = {}\n",
        "      for model_cfg in BASE_MODELS:\n",
        "          name = model_cfg[\"name\"]\n",
        "          key = name.replace(\"/\", \"_\")\n",
        "          for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "              trial_root = os.path.join(TRIALS_DIR, f\"{key}_{mode}\")\n",
        "              if not os.path.exists(trial_root):\n",
        "                  continue\n",
        "              trial_jsons = sorted(glob(os.path.join(trial_root, \"trial_*/metrics.json\")))\n",
        "              trials = [try_load_json(p) for p in trial_jsons if try_load_json(p)]\n",
        "              if trials:\n",
        "                  results[(name, mode)] = trials\n",
        "      return results\n",
        "\n",
        "\n",
        "  def find_trained_models():\n",
        "      trained = []\n",
        "      for model_cfg in BASE_MODELS:\n",
        "          name = model_cfg[\"name\"]\n",
        "          key = name.replace(\"/\", \"_\")\n",
        "          for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "              p = os.path.join(FINAL_DIR, f\"{key}_{mode}_final_trained\")\n",
        "              if os.path.exists(p):\n",
        "                  trained.append((name, mode, p))\n",
        "      return trained\n",
        "\n",
        "\n",
        "  def find_quantized_models():\n",
        "      quant = []\n",
        "      for model_cfg in BASE_MODELS:\n",
        "          name = model_cfg[\"name\"]\n",
        "          key = name.replace(\"/\", \"_\")\n",
        "          for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "              p = os.path.join(FINAL_DIR, f\"{key}_{mode}_quantized_final\")\n",
        "              if os.path.exists(p):\n",
        "                  summary = try_load_json(os.path.join(p, \"quantization_summary.json\"))\n",
        "                  quant.append((name, mode, p, summary))\n",
        "      return quant\n",
        "\n",
        "\n",
        "  # -----------------------------\n",
        "  # Collect experiment artifacts\n",
        "  # -----------------------------\n",
        "  hpo_results = find_hpo_results()\n",
        "  trained_models = find_trained_models()\n",
        "  quantized_models = find_quantized_models()\n",
        "\n",
        "  print(f\"HPO result sets: {len(hpo_results)}\")\n",
        "  print(f\"Trained models: {len(trained_models)}\")\n",
        "  print(f\"Quantized models: {len(quantized_models)}\")\n",
        "\n",
        "  # Build a summary dataframe similar to the dashboard\n",
        "  summary_rows = []\n",
        "  for (model_name, mode), hpo_trials in hpo_results.items():\n",
        "      metric = \"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"\n",
        "      best_trial = max(\n",
        "          hpo_trials,\n",
        "          key=lambda t: t[\"eval_metrics\"].get(metric, -1.0),\n",
        "      )\n",
        "\n",
        "      key = model_name.replace(\"/\", \"_\")\n",
        "      trained_path = os.path.join(FINAL_DIR, f\"{key}_{mode}_final_trained\")\n",
        "      is_trained = os.path.exists(trained_path)\n",
        "\n",
        "      quant_path = os.path.join(FINAL_DIR, f\"{key}_{mode}_quantized_final\")\n",
        "      quant_summary = try_load_json(os.path.join(quant_path, \"quantization_summary.json\"))\n",
        "\n",
        "      summary_rows.append(\n",
        "          {\n",
        "              \"Model\": model_name,\n",
        "              \"Mode\": mode,\n",
        "              \"Best HPO Trial\": best_trial[\"trial_index\"],\n",
        "              \"Best HPO Score\": best_trial[\"eval_metrics\"].get(metric),\n",
        "              \"Trained?\": is_trained,\n",
        "              \"Trained Path\": trained_path if is_trained else None,\n",
        "              \"Quantized?\": quant_summary is not None,\n",
        "              #\"Quantized Score\": quant_summary[\"quantized_metrics\"].get(f\"eval_{metric}\")\n",
        "              \"Quantized Score\": quant_summary[\"quantized_metrics\"].get(metric) if quant_summary else None,\n",
        "              \"Quantized Path\": quant_path if quant_summary else None,\n",
        "          }\n",
        "      )\n",
        "\n",
        "  df_summary = pd.DataFrame(summary_rows) if summary_rows else pd.DataFrame()\n",
        "  summary_csv_path = os.path.join(REPORT_DIR, \"experiment_summary_for_report.csv\")\n",
        "  df_summary.to_csv(summary_csv_path, index=False)\n",
        "  print(\"Saved summary CSV for report:\", summary_csv_path)\n",
        "\n",
        "\n",
        "  # -----------------------------\n",
        "  # Build PDF document\n",
        "  # -----------------------------\n",
        "  styles = getSampleStyleSheet()\n",
        "  style_title = styles[\"Title\"]\n",
        "  style_h1 = styles[\"Heading1\"]\n",
        "  style_h2 = styles[\"Heading2\"]\n",
        "  style_body = styles[\"BodyText\"]\n",
        "\n",
        "  story = []\n",
        "\n",
        "  # ========== TITLE PAGE ==========\n",
        "  title = \"Lightweight Emotion Detection from Text — Final Report\"\n",
        "  today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "  story.append(Paragraph(title, style_title))\n",
        "  story.append(Spacer(1, 24))\n",
        "  story.append(Paragraph(\"Team: (to be filled in)\", style_body))\n",
        "  story.append(Paragraph(f\"Date: {today}\", style_body))\n",
        "  story.append(Spacer(1, 40))\n",
        "\n",
        "  model_names_str = \", \".join(sorted({cfg[\"name\"] for cfg in BASE_MODELS}))\n",
        "  story.append(Paragraph(f\"Models considered: {model_names_str}\", style_body))\n",
        "  story.append(PageBreak())\n",
        "\n",
        "\n",
        "  # ========== 1. OVERVIEW ==========\n",
        "  story.append(Paragraph(\"1. Overview\", style_h1))\n",
        "  overview_text = \"\"\"\n",
        "  This report summarizes our experiments on lightweight emotion detection from text,\n",
        "  using the GoEmotions dataset in two configurations: RAW (multi-label) and SIMPLIFIED\n",
        "  (single-label). The main goals were:\n",
        "\n",
        "  • to compare several compact transformer models on this task,\n",
        "  • to incorporate a Plutchik-distance–aware evaluation instead of plain F1,\n",
        "  • to explore a simple manual hyperparameter search (HPO) grid,\n",
        "  • and to study the impact of INT8 quantization on model performance and size.\n",
        "  \"\"\"\n",
        "  for line in overview_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 2. DATA & LABELLING ==========\n",
        "  story.append(Paragraph(\"2. Data and Labelling Schemes\", style_h1))\n",
        "\n",
        "  raw_text = \"\"\"\n",
        "  RAW configuration:\n",
        "  The RAW GoEmotions configuration is treated as a multi-label problem with 28 active emotion\n",
        "  labels plus 'neutral'. Each text can have multiple emotions simultaneously, represented as\n",
        "  a 28-dimensional multi-hot vector. We use a BCEWithLogitsLoss with per-label pos_weight\n",
        "  to account for class imbalance.\n",
        "  \"\"\"\n",
        "\n",
        "  simp_text = \"\"\"\n",
        "  SIMPLIFIED configuration:\n",
        "  The SIMPLIFIED configuration provided by the dataset authors maps each example to a single\n",
        "  dominant emotion label using a fixed priority rule over the annotations. This turns the problem\n",
        "  into a single-label classification task with 20 emotion classes. For this setting we use a\n",
        "  standard cross-entropy loss, optionally with class weights.\n",
        "  \"\"\"\n",
        "\n",
        "  for t in (raw_text, simp_text):\n",
        "      for line in t.strip().split(\"\\n\"):\n",
        "          story.append(Paragraph(line.strip(), style_body))\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "  plutchik_text = \"\"\"\n",
        "  Plutchik-aware evaluation:\n",
        "  Following your suggestion, we incorporate Plutchik's wheel-of-emotions as a simple distance\n",
        "  model between emotions. For the RAW setting, we define a Plutchik-aware F1 score that gives\n",
        "  partial credit when predicted emotions share the same Plutchik primary category as the true\n",
        "  labels. For the SIMPLIFIED setting, we use a soft accuracy that awards 1.0 for exact matches,\n",
        "  0.5 for same primary category, and 0.25 when one of the emotions is 'neutral'.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in plutchik_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 3. MODELS & TRAINING ==========\n",
        "  story.append(Paragraph(\"3. Models and Training Setup\", style_h1))\n",
        "\n",
        "  models_text = \"\"\"\n",
        "  Models:\n",
        "  We focus on three lightweight transformer-based encoders:\n",
        "\n",
        "  • nreimers/MiniLM-L6-H384-uncased\n",
        "  • google/electra-small-discriminator\n",
        "  • roberta-base\n",
        "\n",
        "  All models are fine-tuned for emotion detection using our custom WeightedTrainer. For the\n",
        "  RAW (multi-label) setting we use BCEWithLogitsLoss with pos_weight; for the SIMPLIFIED\n",
        "  setting, cross-entropy with optional class weights. A global seed is used for reproducibility.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in models_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "  story.append(Spacer(1, 6))\n",
        "\n",
        "  hpo_text = \"\"\"\n",
        "  Manual HPO:\n",
        "  Instead of a black-box hyperparameter search, we use a small manually designed grid of\n",
        "  three configurations (learning rate, warmup ratio, weight decay, scheduler type, batch size,\n",
        "  and max sequence length). For each model we only run HPO on the RAW configuration.\n",
        "  For the SIMPLIFIED setting we simply reuse the best RAW hyperparameters, to save\n",
        "  computation and to keep the comparison consistent.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in hpo_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 4. HPO RESULTS ==========\n",
        "  story.append(Paragraph(\"4. Hyperparameter Optimization Results\", style_h1))\n",
        "\n",
        "  if df_summary.empty:\n",
        "      story.append(Paragraph(\"No HPO results found yet in TRIALS_DIR.\", style_body))\n",
        "  else:\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              \"Table 1: Best HPO result per model and configuration \"\n",
        "              \"(Plutchik-aware score).\",\n",
        "              style_h2,\n",
        "          )\n",
        "      )\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      table_data = [\n",
        "          [\n",
        "              \"Model\",\n",
        "              \"Mode\",\n",
        "              \"Best HPO Trial\",\n",
        "              \"Best HPO Score\",\n",
        "              \"Trained?\",\n",
        "              \"Quantized?\",\n",
        "          ]\n",
        "      ]\n",
        "\n",
        "      for _, row in df_summary.iterrows():\n",
        "          table_data.append(\n",
        "              [\n",
        "                  str(row[\"Model\"]),\n",
        "                  str(row[\"Mode\"]),\n",
        "                  int(row[\"Best HPO Trial\"]),\n",
        "                  f\"{row['Best HPO Score']:.4f}\"\n",
        "                  if pd.notna(row[\"Best HPO Score\"])\n",
        "                  else \"N/A\",\n",
        "                  \"Yes\" if row[\"Trained?\"] else \"No\",\n",
        "                  \"Yes\" if row[\"Quantized?\"] else \"No\",\n",
        "              ]\n",
        "          )\n",
        "\n",
        "      table = Table(table_data, hAlign=\"LEFT\")\n",
        "      table.setStyle(\n",
        "          TableStyle(\n",
        "              [\n",
        "                  (\"BACKGROUND\", (0, 0), (-1, 0), colors.lightgrey),\n",
        "                  (\"GRID\", (0, 0), (-1, -1), 0.5, colors.grey),\n",
        "                  (\"ALIGN\", (0, 0), (-1, -1), \"CENTER\"),\n",
        "                  (\"FONTNAME\", (0, 0), (-1, 0), \"Helvetica-Bold\"),\n",
        "              ]\n",
        "          )\n",
        "      )\n",
        "      story.append(table)\n",
        "\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 5. FINAL TRAINING & PERFORMANCE ==========\n",
        "  story.append(Paragraph(\"5. Final Training and Model Performance\", style_h1))\n",
        "\n",
        "  if not trained_models:\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              \"Final training has not been run yet (no *_final_trained directories found).\",\n",
        "              style_body,\n",
        "          )\n",
        "      )\n",
        "  else:\n",
        "      text = \"\"\"\n",
        "  For each model with available HPO results we perform a final training run using the best\n",
        "  RAW hyperparameters. For the SIMPLIFIED setting we reuse the same hyperparameters.\n",
        "  Performance is evaluated both with the standard metric (micro F1 / accuracy) and our\n",
        "  Plutchik-aware metric.\n",
        "  \"\"\"\n",
        "      for line in text.strip().split(\"\\n\"):\n",
        "          story.append(Paragraph(line.strip(), style_body))\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              \"Training summaries (per model/mode) are stored next to the final trained \"\n",
        "              \"models and can be inspected for detailed metrics and learning curves.\",\n",
        "              style_body,\n",
        "          )\n",
        "      )\n",
        "\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 6. QUANTIZATION ==========\n",
        "  story.append(Paragraph(\"6. Quantization (FP32 vs INT8)\", style_h1))\n",
        "\n",
        "  if not quantized_models:\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              \"No quantized models found yet (no *_quantized_final directories with \"\n",
        "              \"quantization_summary.json).\",\n",
        "              style_body,\n",
        "          )\n",
        "      )\n",
        "  else:\n",
        "      text = \"\"\"\n",
        "  We apply PyTorch dynamic quantization to the best final models, targeting Linear layers\n",
        "  and converting them to INT8 where possible. When quantization fails, we fall back to\n",
        "  exporting the FP32 weights. For each quantized model we re-evaluate the Plutchik-aware\n",
        "  metric to measure the accuracy drop.\n",
        "  \"\"\"\n",
        "      for line in text.strip().split(\"\\n\"):\n",
        "          story.append(Paragraph(line.strip(), style_body))\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      story.append(Paragraph(\"Table 2: Quantization summary.\", style_h2))\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      q_table_data = [\n",
        "          [\"Model\", \"Mode\", \"Quantization OK?\", \"Quantized Score (Plutchik)\"]\n",
        "      ]\n",
        "\n",
        "      for (model_name, mode, p, summary) in quantized_models:\n",
        "          if not summary:\n",
        "              continue\n",
        "          metric = \"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"\n",
        "          #q_val = summary[\"quantized_metrics\"].get(f\"eval_{metric}\")\n",
        "          q_val = summary[\"quantized_metrics\"].get(metric)\n",
        "          q_table_data.append(\n",
        "              [\n",
        "                  model_name,\n",
        "                  mode,\n",
        "                  \"Yes\" if summary.get(\"quantized_success\") else \"No\",\n",
        "                  f\"{q_val:.4f}\" if q_val is not None else \"N/A\",\n",
        "              ]\n",
        "          )\n",
        "\n",
        "      if len(q_table_data) > 1:\n",
        "          q_table = Table(q_table_data, hAlign=\"LEFT\")\n",
        "          q_table.setStyle(\n",
        "              TableStyle(\n",
        "                  [\n",
        "                      (\"BACKGROUND\", (0, 0), (-1, 0), colors.lightgrey),\n",
        "                      (\"GRID\", (0, 0), (-1, -1), 0.5, colors.grey),\n",
        "                      (\"ALIGN\", (0, 0), (-1, -1), \"CENTER\"),\n",
        "                      (\"FONTNAME\", (0, 0), (-1, 0), \"Helvetica-Bold\"),\n",
        "                  ]\n",
        "              )\n",
        "          )\n",
        "          story.append(q_table)\n",
        "      else:\n",
        "          story.append(Paragraph(\"No detailed quantization summaries found.\", style_body))\n",
        "\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 7. RAW vs SIMPLIFIED & MODEL DIFFERENCES ==========\n",
        "  story.append(\n",
        "      Paragraph(\"7. RAW vs SIMPLIFIED, and Model Differences\", style_h1)\n",
        "  )\n",
        "\n",
        "  text_reason = \"\"\"\n",
        "  Difference between best models for RAW and SIMPLIFIED:\n",
        "  In the RAW multi-label setting, the models are optimized to detect possibly several emotions\n",
        "  per example. This tends to favour architectures that handle label co-occurrence patterns\n",
        "  well and make use of the pos_weight vector. In the SIMPLIFIED single-label setup, the task\n",
        "  is closer to standard multi-class classification, and the priority-based label selection\n",
        "  can emphasise different parts of the label space. As a result, the relative ranking of\n",
        "  models can differ between RAW and SIMPLIFIED.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in text_reason.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 8. PERFORMANCE INTERPRETATION ==========\n",
        "  story.append(Paragraph(\"8. Performance Interpretation\", style_h1))\n",
        "\n",
        "  perf_text = \"\"\"\n",
        "  The raw F1 scores in earlier versions of the report were in the 0.47 range, which is modest\n",
        "  for a classification task. However, this is partly due to the multi-label nature of emotions:\n",
        "  expecting exact label-set matches is very strict. With the Plutchik-aware metrics we can\n",
        "  distinguish between 'close' and 'far' mistakes (e.g., confusing joy with optimism vs joy\n",
        "  with disgust), which gives a more informative picture of model behaviour.\n",
        "\n",
        "  We also plan to inspect the main contributors to false positives and false negatives per label\n",
        "  (e.g., which emotions are systematically over-predicted or under-predicted) and to include\n",
        "  a small set of representative error examples in the final written report.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in perf_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 9. COMPUTATIONAL CONSIDERATIONS ==========\n",
        "  story.append(Paragraph(\"9. Computational Considerations\", style_h1))\n",
        "\n",
        "  comp_text = \"\"\"\n",
        "  All experiments are run on Google Colab GPUs, which limits how many models and configurations\n",
        "  we can train in one notebook run. For this reason, we:\n",
        "\n",
        "  • run manual HPO only on the RAW configuration,\n",
        "  • reuse the best RAW hyperparameters for the SIMPLIFIED setting,\n",
        "  • train and quantize models one-by-one by toggling flags at the top of the notebook.\n",
        "\n",
        "  We kept a global random seed and a fixed train/validation split to make results as\n",
        "  reproducible as possible given these constraints.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in comp_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "\n",
        "  story.append(PageBreak())\n",
        "\n",
        "\n",
        "  # ========== 10. APPENDIX: SUMMARY TABLE ==========\n",
        "  story.append(Paragraph(\"10. Appendix: Experiment Summary Table\", style_h1))\n",
        "\n",
        "  if df_summary.empty:\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              \"No summary rows available yet. Please run HPO and training first.\",\n",
        "              style_body,\n",
        "          )\n",
        "      )\n",
        "  else:\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              f\"Full summary also saved as CSV at: {summary_csv_path}\", style_body\n",
        "          )\n",
        "      )\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      # Render a small version of the summary table in the PDF (first N rows)\n",
        "      max_rows = 15\n",
        "      pdf_rows = min(len(df_summary), max_rows)\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              f\"Table 3: Overview of best HPO, training, and quantization status \"\n",
        "              f\"for the first {pdf_rows} configurations.\",\n",
        "              style_h2,\n",
        "          )\n",
        "      )\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      header = list(df_summary.columns)\n",
        "      data = [header]\n",
        "      for i in range(pdf_rows):\n",
        "          row = df_summary.iloc[i]\n",
        "          data.append([str(row[h]) for h in header])\n",
        "\n",
        "      sum_table = Table(data, hAlign=\"LEFT\")\n",
        "      sum_table.setStyle(\n",
        "          TableStyle(\n",
        "              [\n",
        "                  (\"BACKGROUND\", (0, 0), (-1, 0), colors.lightgrey),\n",
        "                  (\"GRID\", (0, 0), (-1, -1), 0.5, colors.grey),\n",
        "                  (\"FONTNAME\", (0, 0), (-1, 0), \"Helvetica-Bold\"),\n",
        "                  (\"ALIGN\", (0, 0), (-1, -1), \"LEFT\"),\n",
        "              ]\n",
        "          )\n",
        "      )\n",
        "      story.append(sum_table)\n",
        "\n",
        "  # -----------------------------\n",
        "  # Build and save PDF\n",
        "  # -----------------------------\n",
        "  print(\"\\nBuilding PDF report...\")\n",
        "  doc = SimpleDocTemplate(REPORT_PDF, pagesize=A4)\n",
        "  doc.build(story)\n",
        "  print(\"✅ Report written to:\", REPORT_PDF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyoUkIGkueyR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cHLVOauuf2D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJQv8bI2ufzM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu2bYSI0ufw_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2x-5eSNufuQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiEHsXJuufrT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYzsKu77LH_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l6QNVBPVLH-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3HYQymViLH8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x0nlnTHFLH5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vsOiUt_4LH3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LfPuWoZcLH1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJtKNNE_LHy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OYfObT-OLHws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# UNIVERSAL EXPERIMENT DASHBOARD\n",
        "#   ✔ Summary table (Pandas dataframe)\n",
        "#   ✔ Saving all plots to Drive\n",
        "#   ✔ Robusztus metric-kezelés (eval_ prefix + fallback)\n",
        "# ============================================================\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "\n",
        "PLOTS_DIR = os.path.join(FINAL_DIR, \"plots\")\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "def savefig(name):\n",
        "    \"\"\"Helper to save current plot to Drive.\"\"\"\n",
        "    path = os.path.join(PLOTS_DIR, name)\n",
        "    plt.savefig(path, bbox_inches=\"tight\", dpi=200)\n",
        "    print(f\"📁 Saved plot: {path}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Helpers\n",
        "# ------------------------------------------------------------\n",
        "def try_load_json(path):\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def get_metric(metrics_dict, metric_name):\n",
        "    \"\"\"\n",
        "    Robusztus metrika-olvasó:\n",
        "    - először próbálja a 'metric_name' kulcsot\n",
        "    - ha nincs, akkor 'eval_metric_name'\n",
        "    \"\"\"\n",
        "    if not metrics_dict:\n",
        "        return None\n",
        "    if metric_name in metrics_dict:\n",
        "        return metrics_dict[metric_name]\n",
        "    eval_key = f\"eval_{metric_name}\"\n",
        "    if eval_key in metrics_dict:\n",
        "        return metrics_dict[eval_key]\n",
        "    return None\n",
        "\n",
        "def find_hpo_results():\n",
        "    results = {}\n",
        "    for model_cfg in BASE_MODELS:\n",
        "        name = model_cfg[\"name\"]\n",
        "        key = name.replace(\"/\", \"_\")\n",
        "        for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "            trial_root = os.path.join(TRIALS_DIR, f\"{key}_{mode}\")\n",
        "            if not os.path.exists(trial_root):\n",
        "                continue\n",
        "            trial_jsons = sorted(glob(os.path.join(trial_root, \"trial_*/metrics.json\")))\n",
        "            trials = [try_load_json(p) for p in trial_jsons if try_load_json(p)]\n",
        "            if trials:\n",
        "                results[(name, mode)] = trials\n",
        "    return results\n",
        "\n",
        "def find_trained_models():\n",
        "    trained = []\n",
        "    for model_cfg in BASE_MODELS:\n",
        "        name = model_cfg[\"name\"]\n",
        "        key = name.replace(\"/\", \"_\")\n",
        "        for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "            p = os.path.join(FINAL_DIR, f\"{key}_{mode}_final_trained\")\n",
        "            if os.path.exists(p):\n",
        "                trained.append((name, mode, p))\n",
        "    return trained\n",
        "\n",
        "def find_quantized_models():\n",
        "    quant = []\n",
        "    for model_cfg in BASE_MODELS:\n",
        "        name = model_cfg[\"name\"]\n",
        "        key = name.replace(\"/\", \"_\")\n",
        "        for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "            p = os.path.join(FINAL_DIR, f\"{key}_{mode}_quantized_final\")\n",
        "            if not os.path.exists(p):\n",
        "                continue\n",
        "            summary = try_load_json(os.path.join(p, \"quantization_summary.json\"))\n",
        "            quant.append((name, mode, p, summary))\n",
        "    return quant\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) PLOT HPO RESULTS (AND SAVE)\n",
        "# ============================================================\n",
        "\n",
        "hpo_results = find_hpo_results()\n",
        "\n",
        "if hpo_results:\n",
        "    print(\"📊 Plotting HPO results...\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    for (model_name, mode), trials in hpo_results.items():\n",
        "        metric = \"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"\n",
        "        xs = [t.get(\"trial_index\", i) for i, t in enumerate(trials)]\n",
        "        ys = [\n",
        "            get_metric(t.get(\"eval_metrics\", {}), metric) or 0.0\n",
        "            for t in trials\n",
        "        ]\n",
        "        plt.plot(xs, ys, marker=\"o\", label=f\"{model_name} | {mode}\")\n",
        "\n",
        "    plt.title(\"HPO Results Across Models\")\n",
        "    plt.xlabel(\"Trial Index\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    savefig(\"hpo_results.png\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠ No HPO results found.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) PLOT TRAINING CURVES (AND SAVE)\n",
        "# ============================================================\n",
        "\n",
        "trained_models = find_trained_models()\n",
        "\n",
        "for (model_name, mode, path) in trained_models:\n",
        "    print(f\"\\n📈 Plotting training curves for {model_name} | {mode}\")\n",
        "\n",
        "    state_path = os.path.join(path, \"trainer_state.json\")\n",
        "    state = try_load_json(state_path)\n",
        "\n",
        "    if not state:\n",
        "        print(\"  ⚠ trainer_state.json missing\")\n",
        "        continue\n",
        "\n",
        "    logs = state.get(\"log_history\", [])\n",
        "    train_loss = [l[\"loss\"] for l in logs if \"loss\" in l]\n",
        "    eval_loss = [l[\"eval_loss\"] for l in logs if \"eval_loss\" in l]\n",
        "\n",
        "    # Training Loss\n",
        "    if train_loss:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(train_loss)\n",
        "        plt.title(f\"Training Loss — {model_name} ({mode})\")\n",
        "        plt.xlabel(\"Step\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.grid(True)\n",
        "        name = f\"training_loss_{model_name.replace('/', '_')}_{mode}.png\"\n",
        "        savefig(name)\n",
        "        plt.show()\n",
        "\n",
        "    # Eval Loss\n",
        "    if eval_loss:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(eval_loss, marker=\"o\")\n",
        "        plt.title(f\"Eval Loss — {model_name} ({mode})\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Eval Loss\")\n",
        "        plt.grid(True)\n",
        "        name = f\"eval_loss_{model_name.replace('/', '_')}_{mode}.png\"\n",
        "        savefig(name)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) FULL vs QUANTIZED PERFORMANCE (AND SAVE)\n",
        "# ============================================================\n",
        "\n",
        "quantized_models = find_quantized_models()\n",
        "\n",
        "for (model_name, mode, p, summary) in quantized_models:\n",
        "    if not summary or \"quantized_metrics\" not in summary:\n",
        "        continue\n",
        "\n",
        "    metric = \"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"\n",
        "    q_val = get_metric(summary[\"quantized_metrics\"], metric)\n",
        "\n",
        "    # Load full model metric\n",
        "    key = model_name.replace(\"/\", \"_\")\n",
        "    full_path = os.path.join(FINAL_DIR, f\"{key}_{mode}_final_trained\")\n",
        "    trainer_state = try_load_json(os.path.join(full_path, \"trainer_state.json\"))\n",
        "\n",
        "    full_val = None\n",
        "    if trainer_state:\n",
        "        eval_logs = [\n",
        "            l for l in trainer_state.get(\"log_history\", [])\n",
        "            if f\"eval_{metric}\" in l\n",
        "        ]\n",
        "        if eval_logs:\n",
        "            full_val = eval_logs[-1][f\"eval_{metric}\"]\n",
        "\n",
        "    if full_val is None or q_val is None:\n",
        "        print(f\"⚠ Missing full/quantized metrics for {model_name} ({mode})\")\n",
        "        continue\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar([\"Full\", \"Quantized\"], [full_val, q_val])\n",
        "    plt.title(f\"{metric}: Full vs Quantized — {model_name} ({mode})\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.grid(axis=\"y\")\n",
        "    name = f\"full_vs_quant_{model_name.replace('/', '_')}_{mode}.png\"\n",
        "    savefig(name)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) SUMMARY TABLE\n",
        "# ============================================================\n",
        "\n",
        "rows = []\n",
        "for (model_name, mode), hpo_trials in hpo_results.items():\n",
        "    metric = \"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"\n",
        "\n",
        "    # best trial robusztus kiválasztása\n",
        "    def trial_score(t):\n",
        "        return get_metric(t.get(\"eval_metrics\", {}), metric) or -1.0\n",
        "\n",
        "    best_trial = max(hpo_trials, key=trial_score)\n",
        "\n",
        "    # trained?\n",
        "    trained_path = os.path.join(\n",
        "        FINAL_DIR,\n",
        "        f\"{model_name.replace('/', '_')}_{mode}_final_trained\"\n",
        "    )\n",
        "    is_trained = os.path.exists(trained_path)\n",
        "\n",
        "    # quantized?\n",
        "    quant_path = os.path.join(\n",
        "        FINAL_DIR,\n",
        "        f\"{model_name.replace('/', '_')}_{mode}_quantized_final\"\n",
        "    )\n",
        "    quant_summary = try_load_json(os.path.join(quant_path, \"quantization_summary.json\"))\n",
        "\n",
        "    quant_score = None\n",
        "    if quant_summary and \"quantized_metrics\" in quant_summary:\n",
        "        quant_score = get_metric(quant_summary[\"quantized_metrics\"], metric)\n",
        "\n",
        "    rows.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Mode\": mode,\n",
        "        \"Best HPO Trial\": best_trial.get(\"trial_index\"),\n",
        "        \"Best HPO Score\": trial_score(best_trial),\n",
        "        \"Trained?\": is_trained,\n",
        "        \"Trained Path\": trained_path if is_trained else None,\n",
        "        \"Quantized?\": quant_summary is not None,\n",
        "        \"Quantized Score\": quant_score,\n",
        "        \"Quantized Path\": quant_path if quant_summary else None,\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(rows)\n",
        "print(\"\\n📘 EXPERIMENT SUMMARY TABLE:\")\n",
        "display(df_summary)\n",
        "\n",
        "# Save summary table\n",
        "df_summary.to_csv(os.path.join(PLOTS_DIR, \"experiment_summary.csv\"), index=False)\n",
        "print(f\"📁 Saved summary table to: {os.path.join(PLOTS_DIR, 'experiment_summary.csv')}\")\n"
      ],
      "metadata": {
        "id": "x-kx2LLmLHuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL REPORT GENERATOR → PDF\n",
        "# Output:\n",
        "#   /content/drive/MyDrive/emotion_project/final_report/final_report.pdf\n",
        "# ============================================================\n",
        "\n",
        "if GENERATE_FINAL_REPORT:\n",
        "  !pip install -q reportlab\n",
        "\n",
        "  import os\n",
        "  import json\n",
        "  import datetime\n",
        "  from glob import glob\n",
        "\n",
        "  import pandas as pd\n",
        "  from reportlab.lib.pagesizes import A4\n",
        "  from reportlab.platypus import (\n",
        "      SimpleDocTemplate,\n",
        "      Paragraph,\n",
        "      Spacer,\n",
        "      Table,\n",
        "      TableStyle,\n",
        "      PageBreak,\n",
        "  )\n",
        "  from reportlab.lib.styles import getSampleStyleSheet\n",
        "  from reportlab.lib import colors\n",
        "\n",
        "  # -----------------------------\n",
        "  # Paths\n",
        "  # -----------------------------\n",
        "  REPORT_DIR = os.path.join(PROJECT_ROOT, \"final_report\")\n",
        "  os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "  REPORT_PDF = os.path.join(REPORT_DIR, \"final_report.pdf\")\n",
        "\n",
        "  print(\"Report directory:\", REPORT_DIR)\n",
        "  print(\"Report PDF will be saved to:\", REPORT_PDF)\n",
        "\n",
        "\n",
        "  # -----------------------------\n",
        "  # Helper functions (reuse logic from dashboard, de lokálisan)\n",
        "  # -----------------------------\n",
        "  def try_load_json(path):\n",
        "      if not os.path.exists(path):\n",
        "          return None\n",
        "      with open(path, \"r\") as f:\n",
        "          return json.load(f)\n",
        "\n",
        "  def get_metric(metrics_dict, metric_name):\n",
        "      \"\"\"\n",
        "      Robusztus metrika-olvasó:\n",
        "      - először próbálja a 'metric_name' kulcsot\n",
        "      - ha nincs, akkor 'eval_metric_name'\n",
        "      \"\"\"\n",
        "      if not metrics_dict:\n",
        "          return None\n",
        "      if metric_name in metrics_dict:\n",
        "          return metrics_dict[metric_name]\n",
        "      eval_key = f\"eval_{metric_name}\"\n",
        "      if eval_key in metrics_dict:\n",
        "          return metrics_dict[eval_key]\n",
        "      return None\n",
        "\n",
        "\n",
        "  def find_hpo_results():\n",
        "      results = {}\n",
        "      for model_cfg in BASE_MODELS:\n",
        "          name = model_cfg[\"name\"]\n",
        "          key = name.replace(\"/\", \"_\")\n",
        "          for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "              trial_root = os.path.join(TRIALS_DIR, f\"{key}_{mode}\")\n",
        "              if not os.path.exists(trial_root):\n",
        "                  continue\n",
        "              trial_jsons = sorted(glob(os.path.join(trial_root, \"trial_*/metrics.json\")))\n",
        "              trials = [try_load_json(p) for p in trial_jsons if try_load_json(p)]\n",
        "              if trials:\n",
        "                  results[(name, mode)] = trials\n",
        "      return results\n",
        "\n",
        "\n",
        "  def find_trained_models():\n",
        "      trained = []\n",
        "      for model_cfg in BASE_MODELS:\n",
        "          name = model_cfg[\"name\"]\n",
        "          key = name.replace(\"/\", \"_\")\n",
        "          for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "              p = os.path.join(FINAL_DIR, f\"{key}_{mode}_final_trained\")\n",
        "              if os.path.exists(p):\n",
        "                  trained.append((name, mode, p))\n",
        "      return trained\n",
        "\n",
        "\n",
        "  def find_quantized_models():\n",
        "      quant = []\n",
        "      for model_cfg in BASE_MODELS:\n",
        "          name = model_cfg[\"name\"]\n",
        "          key = name.replace(\"/\", \"_\")\n",
        "          for mode in [RAW_CONFIG_NAME, SIMPLIFIED_CONFIG_NAME]:\n",
        "              p = os.path.join(FINAL_DIR, f\"{key}_{mode}_quantized_final\")\n",
        "              if not os.path.exists(p):\n",
        "                  continue\n",
        "              summary = try_load_json(os.path.join(p, \"quantization_summary.json\"))\n",
        "              quant.append((name, mode, p, summary))\n",
        "      return quant\n",
        "\n",
        "\n",
        "  # -----------------------------\n",
        "  # Collect experiment artifacts\n",
        "  # -----------------------------\n",
        "  hpo_results = find_hpo_results()\n",
        "  trained_models = find_trained_models()\n",
        "  quantized_models = find_quantized_models()\n",
        "\n",
        "  print(f\"HPO result sets: {len(hpo_results)}\")\n",
        "  print(f\"Trained models: {len(trained_models)}\")\n",
        "  print(f\"Quantized models: {len(quantized_models)}\")\n",
        "\n",
        "  # Build a summary dataframe similar to the dashboard\n",
        "  summary_rows = []\n",
        "  for (model_name, mode), hpo_trials in hpo_results.items():\n",
        "      metric = \"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"\n",
        "\n",
        "      def trial_score(t):\n",
        "          return get_metric(t.get(\"eval_metrics\", {}), metric) or -1.0\n",
        "\n",
        "      best_trial = max(\n",
        "          hpo_trials,\n",
        "          key=trial_score,\n",
        "      )\n",
        "\n",
        "      key = model_name.replace(\"/\", \"_\")\n",
        "      trained_path = os.path.join(FINAL_DIR, f\"{key}_{mode}_final_trained\")\n",
        "      is_trained = os.path.exists(trained_path)\n",
        "\n",
        "      quant_path = os.path.join(FINAL_DIR, f\"{key}_{mode}_quantized_final\")\n",
        "      quant_summary = try_load_json(os.path.join(quant_path, \"quantization_summary.json\"))\n",
        "\n",
        "      quant_score = None\n",
        "      if quant_summary and \"quantized_metrics\" in quant_summary:\n",
        "          quant_score = get_metric(quant_summary[\"quantized_metrics\"], metric)\n",
        "\n",
        "      summary_rows.append(\n",
        "          {\n",
        "              \"Model\": model_name,\n",
        "              \"Mode\": mode,\n",
        "              \"Best HPO Trial\": best_trial.get(\"trial_index\"),\n",
        "              \"Best HPO Score\": trial_score(best_trial),\n",
        "              \"Trained?\": is_trained,\n",
        "              \"Trained Path\": trained_path if is_trained else None,\n",
        "              \"Quantized?\": quant_summary is not None,\n",
        "              \"Quantized Score\": quant_score,\n",
        "              \"Quantized Path\": quant_path if quant_summary else None,\n",
        "          }\n",
        "      )\n",
        "\n",
        "  df_summary = pd.DataFrame(summary_rows) if summary_rows else pd.DataFrame()\n",
        "  summary_csv_path = os.path.join(REPORT_DIR, \"experiment_summary_for_report.csv\")\n",
        "  df_summary.to_csv(summary_csv_path, index=False)\n",
        "  print(\"Saved summary CSV for report:\", summary_csv_path)\n",
        "\n",
        "\n",
        "  # -----------------------------\n",
        "  # Build PDF document\n",
        "  # -----------------------------\n",
        "  styles = getSampleStyleSheet()\n",
        "  style_title = styles[\"Title\"]\n",
        "  style_h1 = styles[\"Heading1\"]\n",
        "  style_h2 = styles[\"Heading2\"]\n",
        "  style_body = styles[\"BodyText\"]\n",
        "\n",
        "  story = []\n",
        "\n",
        "  # ========== TITLE PAGE ==========\n",
        "  title = \"Lightweight Emotion Detection from Text — Final Report\"\n",
        "  today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "  story.append(Paragraph(title, style_title))\n",
        "  story.append(Spacer(1, 24))\n",
        "  story.append(Paragraph(\"Team: (to be filled in)\", style_body))\n",
        "  story.append(Paragraph(f\"Date: {today}\", style_body))\n",
        "  story.append(Spacer(1, 40))\n",
        "\n",
        "  model_names_str = \", \".join(sorted({cfg[\"name\"] for cfg in BASE_MODELS}))\n",
        "  story.append(Paragraph(f\"Models considered: {model_names_str}\", style_body))\n",
        "  story.append(PageBreak())\n",
        "\n",
        "\n",
        "  # ========== 1. OVERVIEW ==========\n",
        "  story.append(Paragraph(\"1. Overview\", style_h1))\n",
        "  overview_text = \"\"\"\n",
        "  This report summarizes our experiments on lightweight emotion detection from text,\n",
        "  using the GoEmotions dataset in two configurations: RAW (multi-label) and SIMPLIFIED\n",
        "  (single-label). The main goals were:\n",
        "\n",
        "  • to compare several compact transformer models on this task,\n",
        "  • to incorporate a Plutchik-distance–aware evaluation instead of plain F1,\n",
        "  • to explore a simple manual hyperparameter search (HPO) grid,\n",
        "  • and to study the impact of INT8 quantization on model performance and size.\n",
        "  \"\"\"\n",
        "  for line in overview_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 2. DATA & LABELLING ==========\n",
        "  story.append(Paragraph(\"2. Data and Labelling Schemes\", style_h1))\n",
        "\n",
        "  raw_text = \"\"\"\n",
        "  RAW configuration:\n",
        "  The RAW GoEmotions configuration is treated as a multi-label problem with 28 active emotion\n",
        "  labels plus 'neutral'. Each text can have multiple emotions simultaneously, represented as\n",
        "  a 28-dimensional multi-hot vector. We use a BCEWithLogitsLoss with per-label pos_weight\n",
        "  to account for class imbalance.\n",
        "  \"\"\"\n",
        "\n",
        "  simp_text = \"\"\"\n",
        "  SIMPLIFIED configuration:\n",
        "  The SIMPLIFIED configuration provided by the dataset authors maps each example to a single\n",
        "  dominant emotion label using a fixed priority rule over the annotations. This turns the problem\n",
        "  into a single-label classification task with 20 emotion classes. For this setting we use a\n",
        "  standard cross-entropy loss, optionally with class weights.\n",
        "  \"\"\"\n",
        "\n",
        "  for t in (raw_text, simp_text):\n",
        "      for line in t.strip().split(\"\\n\"):\n",
        "          story.append(Paragraph(line.strip(), style_body))\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "  plutchik_text = \"\"\"\n",
        "  Plutchik-aware evaluation:\n",
        "  Following your suggestion, we incorporate Plutchik's wheel-of-emotions as a simple distance\n",
        "  model between emotions. For the RAW setting, we define a Plutchik-aware F1 score that gives\n",
        "  partial credit when predicted emotions share the same Plutchik primary category as the true\n",
        "  labels. For the SIMPLIFIED setting, we use a soft accuracy that awards 1.0 for exact matches,\n",
        "  0.5 for same primary category, and 0.25 when one of the emotions is 'neutral'.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in plutchik_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 3. MODELS & TRAINING ==========\n",
        "  story.append(Paragraph(\"3. Models and Training Setup\", style_h1))\n",
        "\n",
        "  models_text = \"\"\"\n",
        "  Models:\n",
        "  We focus on three lightweight transformer-based encoders:\n",
        "\n",
        "  • nreimers/MiniLM-L6-H384-uncased\n",
        "  • google/electra-small-discriminator\n",
        "  • roberta-base\n",
        "\n",
        "  All models are fine-tuned for emotion detection using our custom WeightedTrainer. For the\n",
        "  RAW (multi-label) setting we use BCEWithLogitsLoss with pos_weight; for the SIMPLIFIED\n",
        "  setting, cross-entropy with optional class weights. A global seed is used for reproducibility.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in models_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "  story.append(Spacer(1, 6))\n",
        "\n",
        "  hpo_text = \"\"\"\n",
        "  Manual HPO:\n",
        "  Instead of a black-box hyperparameter search, we use a small manually designed grid of\n",
        "  three configurations (learning rate, warmup ratio, weight decay, scheduler type, batch size,\n",
        "  and max sequence length). For each model we only run HPO on the RAW configuration.\n",
        "  For the SIMPLIFIED setting we simply reuse the best RAW hyperparameters, to save\n",
        "  computation and to keep the comparison consistent.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in hpo_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 4. HPO RESULTS ==========\n",
        "  story.append(Paragraph(\"4. Hyperparameter Optimization Results\", style_h1))\n",
        "\n",
        "  if df_summary.empty:\n",
        "      story.append(Paragraph(\"No HPO results found yet in TRIALS_DIR.\", style_body))\n",
        "  else:\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              \"Table 1: Best HPO result per model and configuration \"\n",
        "              \"(Plutchik-aware score).\",\n",
        "              style_h2,\n",
        "          )\n",
        "      )\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      table_data = [\n",
        "          [\n",
        "              \"Model\",\n",
        "              \"Mode\",\n",
        "              \"Best HPO Trial\",\n",
        "              \"Best HPO Score\",\n",
        "              \"Trained?\",\n",
        "              \"Quantized?\",\n",
        "          ]\n",
        "      ]\n",
        "\n",
        "      for _, row in df_summary.iterrows():\n",
        "          best_score = row[\"Best HPO Score\"]\n",
        "          table_data.append(\n",
        "              [\n",
        "                  str(row[\"Model\"]),\n",
        "                  str(row[\"Mode\"]),\n",
        "                  int(row[\"Best HPO Trial\"]) if pd.notna(row[\"Best HPO Trial\"]) else \"N/A\",\n",
        "                  f\"{best_score:.4f}\" if pd.notna(best_score) else \"N/A\",\n",
        "                  \"Yes\" if row[\"Trained?\"] else \"No\",\n",
        "                  \"Yes\" if row[\"Quantized?\"] else \"No\",\n",
        "              ]\n",
        "          )\n",
        "\n",
        "      table = Table(table_data, hAlign=\"LEFT\")\n",
        "      table.setStyle(\n",
        "          TableStyle(\n",
        "              [\n",
        "                  (\"BACKGROUND\", (0, 0), (-1, 0), colors.lightgrey),\n",
        "                  (\"GRID\", (0, 0), (-1, -1), 0.5, colors.grey),\n",
        "                  (\"ALIGN\", (0, 0), (-1, -1), \"CENTER\"),\n",
        "                  (\"FONTNAME\", (0, 0), (-1, 0), \"Helvetica-Bold\"),\n",
        "              ]\n",
        "          )\n",
        "      )\n",
        "      story.append(table)\n",
        "\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 5. FINAL TRAINING & PERFORMANCE ==========\n",
        "  story.append(Paragraph(\"5. Final Training and Model Performance\", style_h1))\n",
        "\n",
        "  if not trained_models:\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              \"Final training has not been run yet (no *_final_trained directories found).\",\n",
        "              style_body,\n",
        "          )\n",
        "      )\n",
        "  else:\n",
        "      text = \"\"\"\n",
        "  For each model with available HPO results we perform a final training run using the best\n",
        "  RAW hyperparameters. For the SIMPLIFIED setting we reuse the same hyperparameters.\n",
        "  Performance is evaluated both with the standard metric (micro F1 / accuracy) and our\n",
        "  Plutchik-aware metric.\n",
        "  \"\"\"\n",
        "      for line in text.strip().split(\"\\n\"):\n",
        "          story.append(Paragraph(line.strip(), style_body))\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              \"Training summaries (per model/mode) are stored next to the final trained \"\n",
        "              \"models and can be inspected for detailed metrics and learning curves.\",\n",
        "              style_body,\n",
        "          )\n",
        "      )\n",
        "\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 6. QUANTIZATION ==========\n",
        "  story.append(Paragraph(\"6. Quantization (FP32 vs INT8)\", style_h1))\n",
        "\n",
        "  if not quantized_models:\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              \"No quantized models found yet (no *_quantized_final directories with \"\n",
        "              \"quantization_summary.json).\",\n",
        "              style_body,\n",
        "          )\n",
        "      )\n",
        "  else:\n",
        "      text = \"\"\"\n",
        "  We apply PyTorch dynamic quantization to the best final models, targeting Linear layers\n",
        "  and converting them to INT8 where possible. When quantization fails, we fall back to\n",
        "  exporting the FP32 weights. For each quantized model we re-evaluate the Plutchik-aware\n",
        "  metric to measure the accuracy drop.\n",
        "  \"\"\"\n",
        "      for line in text.strip().split(\"\\n\"):\n",
        "          story.append(Paragraph(line.strip(), style_body))\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      story.append(Paragraph(\"Table 2: Quantization summary.\", style_h2))\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      q_table_data = [\n",
        "          [\"Model\", \"Mode\", \"Quantization OK?\", \"Quantized Score (Plutchik)\"]\n",
        "      ]\n",
        "\n",
        "      for (model_name, mode, p, summary) in quantized_models:\n",
        "          if not summary or \"quantized_metrics\" not in summary:\n",
        "              continue\n",
        "          metric = \"plutchik_f1\" if mode == RAW_CONFIG_NAME else \"plutchik_soft\"\n",
        "          q_val = get_metric(summary[\"quantized_metrics\"], metric)\n",
        "          q_table_data.append(\n",
        "              [\n",
        "                  model_name,\n",
        "                  mode,\n",
        "                  \"Yes\" if summary.get(\"quantized_success\") else \"No\",\n",
        "                  f\"{q_val:.4f}\" if q_val is not None else \"N/A\",\n",
        "              ]\n",
        "          )\n",
        "\n",
        "      if len(q_table_data) > 1:\n",
        "          q_table = Table(q_table_data, hAlign=\"LEFT\")\n",
        "          q_table.setStyle(\n",
        "              TableStyle(\n",
        "                  [\n",
        "                      (\"BACKGROUND\", (0, 0), (-1, 0), colors.lightgrey),\n",
        "                      (\"GRID\", (0, 0), (-1, -1), 0.5, colors.grey),\n",
        "                      (\"ALIGN\", (0, 0), (-1, -1), \"CENTER\"),\n",
        "                      (\"FONTNAME\", (0, 0), (-1, 0), \"Helvetica-Bold\"),\n",
        "                  ]\n",
        "              )\n",
        "          )\n",
        "          story.append(q_table)\n",
        "      else:\n",
        "          story.append(Paragraph(\"No detailed quantization summaries found.\", style_body))\n",
        "\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 7. RAW vs SIMPLIFIED & MODEL DIFFERENCES ==========\n",
        "  story.append(\n",
        "      Paragraph(\"7. RAW vs SIMPLIFIED, and Model Differences\", style_h1)\n",
        "  )\n",
        "\n",
        "  text_reason = \"\"\"\n",
        "  Difference between best models for RAW and SIMPLIFIED:\n",
        "  In the RAW multi-label setting, the models are optimized to detect possibly several emotions\n",
        "  per example. This tends to favour architectures that handle label co-occurrence patterns\n",
        "  well and make use of the pos_weight vector. In the SIMPLIFIED single-label setup, the task\n",
        "  is closer to standard multi-class classification, and the priority-based label selection\n",
        "  can emphasise different parts of the label space. As a result, the relative ranking of\n",
        "  models can differ between RAW and SIMPLIFIED.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in text_reason.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 8. PERFORMANCE INTERPRETATION ==========\n",
        "  story.append(Paragraph(\"8. Performance Interpretation\", style_h1))\n",
        "\n",
        "  perf_text = \"\"\"\n",
        "  The raw F1 scores in earlier versions of the report were in the 0.47 range, which is modest\n",
        "  for a classification task. However, this is partly due to the multi-label nature of emotions:\n",
        "  expecting exact label-set matches is very strict. With the Plutchik-aware metrics we can\n",
        "  distinguish between 'close' and 'far' mistakes (e.g., confusing joy with optimism vs joy\n",
        "  with disgust), which gives a more informative picture of model behaviour.\n",
        "\n",
        "  We also plan to inspect the main contributors to false positives and false negatives per label\n",
        "  (e.g., which emotions are systematically over-predicted or under-predicted) and to include\n",
        "  a small set of representative error examples in the final written report.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in perf_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "\n",
        "  story.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "  # ========== 9. COMPUTATIONAL CONSIDERATIONS ==========\n",
        "  story.append(Paragraph(\"9. Computational Considerations\", style_h1))\n",
        "\n",
        "  comp_text = \"\"\"\n",
        "  All experiments are run on Google Colab GPUs, which limits how many models and configurations\n",
        "  we can train in one notebook run. For this reason, we:\n",
        "\n",
        "  • run manual HPO only on the RAW configuration,\n",
        "  • reuse the best RAW hyperparameters for the SIMPLIFIED setting,\n",
        "  • train and quantize models one-by-one by toggling flags at the top of the notebook.\n",
        "\n",
        "  We kept a global random seed and a fixed train/validation split to make results as\n",
        "  reproducible as possible given these constraints.\n",
        "  \"\"\"\n",
        "\n",
        "  for line in comp_text.strip().split(\"\\n\"):\n",
        "      story.append(Paragraph(line.strip(), style_body))\n",
        "\n",
        "  story.append(PageBreak())\n",
        "\n",
        "\n",
        "  # ========== 10. APPENDIX: SUMMARY TABLE ==========\n",
        "  story.append(Paragraph(\"10. Appendix: Experiment Summary Table\", style_h1))\n",
        "\n",
        "  if df_summary.empty:\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              \"No summary rows available yet. Please run HPO and training first.\",\n",
        "              style_body,\n",
        "          )\n",
        "      )\n",
        "  else:\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              f\"Full summary also saved as CSV at: {summary_csv_path}\", style_body\n",
        "          )\n",
        "      )\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      # Render a small version of the summary table in the PDF (first N rows)\n",
        "      max_rows = 15\n",
        "      pdf_rows = min(len(df_summary), max_rows)\n",
        "      story.append(\n",
        "          Paragraph(\n",
        "              f\"Table 3: Overview of best HPO, training, and quantization status \"\n",
        "              f\"for the first {pdf_rows} configurations.\",\n",
        "              style_h2,\n",
        "          )\n",
        "      )\n",
        "      story.append(Spacer(1, 6))\n",
        "\n",
        "      header = list(df_summary.columns)\n",
        "      data = [header]\n",
        "      for i in range(pdf_rows):\n",
        "          row = df_summary.iloc[i]\n",
        "          data.append([str(row[h]) for h in header])\n",
        "\n",
        "      sum_table = Table(data, hAlign=\"LEFT\")\n",
        "      sum_table.setStyle(\n",
        "          TableStyle(\n",
        "              [\n",
        "                  (\"BACKGROUND\", (0, 0), (-1, 0), colors.lightgrey),\n",
        "                  (\"GRID\", (0, 0), (-1, -1), 0.5, colors.grey),\n",
        "                  (\"FONTNAME\", (0, 0), (-1, 0), \"Helvetica-Bold\"),\n",
        "                  (\"ALIGN\", (0, 0), (-1, -1), \"LEFT\"),\n",
        "              ]\n",
        "          )\n",
        "      )\n",
        "      story.append(sum_table)\n",
        "\n",
        "  # -----------------------------\n",
        "  # Build and save PDF\n",
        "  # -----------------------------\n",
        "  print(\"\\nBuilding PDF report...\")\n",
        "  doc = SimpleDocTemplate(REPORT_PDF, pagesize=A4)\n",
        "  doc.build(story)\n",
        "  print(\"✅ Report written to:\", REPORT_PDF)\n"
      ],
      "metadata": {
        "id": "tSpBMn5BLHrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}